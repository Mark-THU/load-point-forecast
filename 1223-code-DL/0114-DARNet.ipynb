{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6210c310",
   "metadata": {},
   "source": [
    "# DARNet for power load forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e8376",
   "metadata": {},
   "source": [
    "- [x] 实现\n",
    "- [x] 所有cnn-rnn层都有rnn快速通道，rnn-cnn,decoder中rnn没有dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbaaa556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.770100Z",
     "start_time": "2022-01-19T07:44:38.760783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a48fb06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.778034Z",
     "start_time": "2022-01-19T07:44:38.773159Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_seed_set(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.random.manual_seed(seed)\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d6cb62",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba489706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.838481Z",
     "start_time": "2022-01-19T07:44:38.780604Z"
    }
   },
   "outputs": [],
   "source": [
    "url = '../data/beijing.csv'\n",
    "data = pd.read_csv(url, sep=',', index_col='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99431b5",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3081c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.842659Z",
     "start_time": "2022-01-19T07:44:38.839896Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    \"\"\"\n",
    "    data: original data with load\n",
    "    return: normalized data, scaler of load\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit_transform(data[[data.columns[-1]]])\n",
    "    return normalized_data, scaler, scaler_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15650a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T07:45:44.192666Z",
     "start_time": "2021-12-22T07:45:44.188985Z"
    }
   },
   "source": [
    "## build supervised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eff9fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.847343Z",
     "start_time": "2022-01-19T07:44:38.843647Z"
    }
   },
   "outputs": [],
   "source": [
    "def series_to_supervise(data, seq_len, target_len):\n",
    "    \"\"\"\n",
    "    convert series data to supervised data\n",
    "    :param data: original data\n",
    "    :param seq_len: length of input sequence\n",
    "    :param target_len: length of ouput sequence\n",
    "    :return: return two ndarrays-- input and output in format suitable to feed to RNN\n",
    "    \"\"\"\n",
    "    dim_0 = data.shape[0] - seq_len - target_len + 1\n",
    "    dim_1 = data.shape[1]\n",
    "    x = np.zeros((dim_0, seq_len, dim_1))\n",
    "    y = np.zeros((dim_0, target_len, dim_1))\n",
    "    for i in range(dim_0):\n",
    "        x[i] = data[i:i + seq_len]\n",
    "        y[i] = data[i + seq_len:i + seq_len + target_len]\n",
    "    print(\"supervised data: shape of x: {}, shape of y: {}\".format(\n",
    "        x.shape, y.shape))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03794ff6",
   "metadata": {},
   "source": [
    "## 5-folds TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0df9a8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.851797Z",
     "start_time": "2022-01-19T07:44:38.848533Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_series_split(X, Y, n_split=5):\n",
    "    \"\"\"\n",
    "    X: features, size * seq_len * feature_num\n",
    "    Y: labels, size * target_len\n",
    "    return: list of train_x, test_x, train_y, test_y\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    train_x_list = list()\n",
    "    valid_x_list = list()\n",
    "    train_y_list = list()\n",
    "    valid_y_list = list()\n",
    "    for train_index, valid_index in tscv.split(X):\n",
    "        train_x_list.append(X[train_index])\n",
    "        train_y_list.append(Y[train_index])\n",
    "        valid_x_list.append(X[valid_index])\n",
    "        valid_y_list.append(Y[valid_index])\n",
    "    return train_x_list, train_y_list, valid_x_list, valid_y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3f74a",
   "metadata": {},
   "source": [
    "## DARNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf95810",
   "metadata": {},
   "source": [
    "### CNN1D-RNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9961f7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.857897Z",
     "start_time": "2022-01-19T07:44:38.852738Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN1D_RNN_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout, kernel_size=3):\n",
    "        super(CNN1D_RNN_Block, self).__init__()\n",
    "        # params\n",
    "        padding = int(kernel_size / 2)\n",
    "\n",
    "        # layers\n",
    "        self.CNN = nn.Conv1d(in_channels,\n",
    "                             out_channels,\n",
    "                             kernel_size,\n",
    "                             padding=padding)\n",
    "        self.RNN1 = nn.GRU(input_size=out_channels,\n",
    "                           hidden_size=out_channels,\n",
    "                           num_layers=2,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.RNN2 = nn.GRU(input_size=in_channels,\n",
    "                           hidden_size=out_channels,\n",
    "                           num_layers=2,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x shape (batch_size, seq_len, d_feature)\n",
    "        '''\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        cnn_input = x.permute(0, 2, 1)\n",
    "        # cnn_input shape (batch_size, in_channel, seq_len)\n",
    "        cnn_out = self.CNN(cnn_input)\n",
    "        # cnn_out = self.bn1(cnn_out)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        # cnn_out shape (batch_size, out_channel, seq_len)\n",
    "\n",
    "        rnn_input = cnn_out.permute(0, 2, 1)\n",
    "        # rnn_input shape (batch_size, seq_len, out_channels)\n",
    "        rnn_out_1, _ = self.RNN1(rnn_input)\n",
    "        # rnn_out_1 shape (batch_size, seq_len, out_channels)\n",
    "\n",
    "        rnn_out_2, _ = self.RNN2(x)\n",
    "        # rnn_out_2 shape (batch_size, seq_len, out_channels)\n",
    "\n",
    "        out = self.relu(rnn_out_1 + rnn_out_2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de044425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:46:28.994825Z",
     "start_time": "2022-01-14T09:46:28.959916Z"
    }
   },
   "source": [
    "### RNN-CNN2D Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "306ff680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.863185Z",
     "start_time": "2022-01-19T07:44:38.858859Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN_CNN2D_Block(nn.Module):\n",
    "    def __init__(self, input_size, out_channels, seq_len, dropout, n_layers=2):\n",
    "        super(RNN_CNN2D_Block, self).__init__()\n",
    "\n",
    "        # layers\n",
    "        self.RNN = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=out_channels,\n",
    "                          num_layers=n_layers,\n",
    "                          dropout=dropout,\n",
    "                          batch_first=True)\n",
    "        self.CNN = nn.Conv2d(in_channels=1,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=(seq_len, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x shape (batch_size, seq_len, input_size)\n",
    "        '''\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        rnn_out, hidden_state = self.RNN(x)\n",
    "        # rnn_out shape (batch_size, seq_len, d_features)\n",
    "\n",
    "        cnn_input = rnn_out.unsqueeze(1)\n",
    "        # cnn_input shape (batch_size, 1, seq_len, d_featues)\n",
    "        cnn_out = self.CNN(cnn_input)\n",
    "        # cnn_out shape (batch_size, out_channels, 1, d_features)\n",
    "        cnn_out = cnn_out.squeeze(2).permute(0, 2, 1)\n",
    "        # cnn_out = self.bn2(cnn_out)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        \n",
    "        # cnn_out shape (batch_size, d_features, out_channels)\n",
    "\n",
    "        return cnn_out, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45dd894",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1060b97e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.869459Z",
     "start_time": "2022-01-19T07:44:38.864115Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, seq_len, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        '''\n",
    "        input_size(int): dimension of features\n",
    "        num_channels(list): channels of each cnn-rnn layer\n",
    "        seq_len(int): window length of input\n",
    "        '''\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            in_channels = input_size if i==0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [CNN1D_RNN_Block(in_channels, out_channels, dropout)]\n",
    "    \n",
    "        self.cnn_rnn = nn.Sequential(*layers)\n",
    "        self.rnn_cnn = RNN_CNN2D_Block(input_size, num_channels[-1], seq_len, dropout=0)\n",
    "    \n",
    "    def forward(self, x, rho):\n",
    "        '''\n",
    "        x shape (batch_size, seq_len, input_size)\n",
    "        '''\n",
    "        features = x[:, :, :-1]\n",
    "        values = x[:, :, -1]\n",
    "        # features shape (batch_size, num_steps, input_size -1)\n",
    "        # values shape (batch_size, num_steps)\n",
    "\n",
    "        values_mean = torch.mean(values, dim=1)\n",
    "        # values_mean shape (batch_size)\n",
    "        values = torch.cat((values_mean.unsqueeze(1), values), dim=1)\n",
    "        # values shape (batch_size, num_steps + 1)\n",
    "        values = values[:, 1:] - rho * values[:, :-1]\n",
    "        # values shape (batch_size, num_steps)\n",
    "        inp = torch.cat((features, values.unsqueeze(2)), dim=2)\n",
    "        # inp shape (batch_size, num_steps, input_size)\n",
    "        \n",
    "        cnn_rnn_out = self.cnn_rnn(inp)\n",
    "        # cnn_rnn_out shape (batch_size, seq_len, num_channels[-1])\n",
    "        \n",
    "        rnn_cnn_out, hidden_state = self.rnn_cnn(inp)\n",
    "        # rnn_cnn_out shape (batch_size, num_channels[-1], num_channels[-1])\n",
    "        \n",
    "        return cnn_rnn_out, rnn_cnn_out, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39cfb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-31T13:15:30.991666Z",
     "start_time": "2021-12-31T13:15:30.987285Z"
    }
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "561a01f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.889858Z",
     "start_time": "2022-01-19T07:44:38.871140Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # 在维度扩展后，\n",
    "        # `queries` 的形状：(`batch_size`，查询的个数，1，`num_hidden`)\n",
    "        # `key` 的形状：(`batch_size`，1，“键－值”对的个数，`num_hiddens`)\n",
    "        # 使用广播方式进行求和\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # `self.w_v` 仅有一个输出，因此从形状中移除最后那个维度。\n",
    "        # `scores` 的形状：(`batch_size`，查询的个数，“键-值”对的个数)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        # `values` 的形状：(`batch_size`，“键－值”对的个数，值的维度)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # `queries` 的形状：(`batch_size`，查询的个数，`d`)\n",
    "    # `keys` 的形状：(`batch_size`，“键－值”对的个数，`d`)\n",
    "    # `values` 的形状：(`batch_size`，“键－值”对的个数，值的维度)\n",
    "    # `valid_lens` 的形状: (`batch_size`，) 或者 (`batch_size`，查询的个数)\n",
    "    def forward(self, queries, keys, values):\n",
    "        d = queries.shape[-1]\n",
    "        # 设置 `transpose_b=True` 为了交换 `keys` 的最后两个维度\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self,\n",
    "                 key_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hiddens,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 bias=False,\n",
    "                 **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # `queries`，`keys`，`values` 的形状:\n",
    "        # (`batch_size`，查询或者“键－值”对的个数，`num_hiddens`)\n",
    "        # `valid_lens`　的形状:\n",
    "        # (`batch_size`，) 或 (`batch_size`，查询的个数)\n",
    "        # 经过变换后，输出的 `queries`，`keys`，`values`　的形状:\n",
    "        # (`batch_size` * `num_heads`，查询或者“键－值”对的个数，\n",
    "        # `num_hiddens` / `num_heads`)\n",
    "        queries = transpose_qkv(queries, self.num_heads)\n",
    "        keys = transpose_qkv(keys, self.num_heads)\n",
    "        values = transpose_qkv(values, self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴 0，将第一项（标量或者矢量）复制 `num_heads` 次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(valid_lens,\n",
    "                                                 repeats=self.num_heads,\n",
    "                                                 dim=0)\n",
    "\n",
    "        # `output` 的形状: (`batch_size` * `num_heads`，查询的个数，\n",
    "        # `num_hiddens` / `num_heads`)\n",
    "        output = self.attention(queries, keys, values)\n",
    "\n",
    "        # `output_concat` 的形状: (`batch_size`，查询的个数，`num_hiddens`)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状。\"\"\"\n",
    "    # 输入 `X` 的形状: (`batch_size`，查询或者“键－值”对的个数，`num_hiddens`)\n",
    "    # 输出 `X` 的形状: (`batch_size`，查询或者“键－值”对的个数，`num_heads`，\n",
    "    # `num_hiddens` / `num_heads`)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # 输出 `X` 的形状: (`batch_size`，`num_heads`，查询或者“键－值”对的个数,\n",
    "    # `num_hiddens` / `num_heads`)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状: (`batch_size` * `num_heads`, 查询或者“键－值”对的个数,\n",
    "    # `num_hiddens` / `num_heads`)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转 `transpose_qkv` 函数的操作。\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98694b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T02:15:31.953406Z",
     "start_time": "2022-01-16T02:15:31.948802Z"
    }
   },
   "source": [
    "### RNN-Attn Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "702f4ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T09:11:19.929391Z",
     "start_time": "2022-01-19T09:11:19.911631Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN_Attn_Block(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, i, dropout=0.5):\n",
    "        super(RNN_Attn_Block, self).__init__()\n",
    "        # params\n",
    "        self.i = i\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # layers\n",
    "        self.RNN = nn.GRU(input_size=input_size + hidden_dim,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout)\n",
    "        self.attention1 = DotProductAttention(dropout)\n",
    "        self.attention2 = DotProductAttention(dropout)\n",
    "        #self.attention1 = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, 2, dropout)\n",
    "        #self.attention2 = MultiHeadAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, 2, dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        '''\n",
    "        x shape (batch_size, 1, input_size)\n",
    "        state[0] cnn_rnn_out shape (batch_size, 72, hidden_dim)\n",
    "        state[1] rnn_cnn_out shape (batch_size, hidden_dim, hidden_dim)\n",
    "        state[2][i] shape (num_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        cnn_rnn_out = state[0]\n",
    "        rnn_cnn_out = state[1]\n",
    "\n",
    "        query = state[2][self.i][-1].unsqueeze(1)\n",
    "        # query shape (batch_size, 1, hidden_dim)\n",
    "\n",
    "        context_1 = self.attention1(query, cnn_rnn_out, cnn_rnn_out)\n",
    "        # context_1 shape (batch_size, 1, hidden_dim)\n",
    "\n",
    "        rnn_input = torch.cat((x, context_1), dim=-1)\n",
    "        rnn_out, state[2][self.i] = self.RNN(rnn_input, state[2][self.i])\n",
    "        # rnn_out shape (batch_size, 1, hidden_dim)\n",
    "        # state shape (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        context_2 = self.attention2(rnn_out, rnn_cnn_out, rnn_cnn_out)\n",
    "        # context_2 shape (batch_size, 1, hidden_dim)\n",
    "\n",
    "        out = self.dense(torch.cat((rnn_out, context_2), dim=-1))\n",
    "        if self.input_size != self.hidden_dim:\n",
    "            out = self.relu(out + rnn_out)\n",
    "        # out shape (batch_size, 1, hidden_dim)\n",
    "        else:\n",
    "            out = self.relu(out + x)\n",
    "\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f88ea",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c89a0fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.902640Z",
     "start_time": "2022-01-19T07:44:38.897914Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_dim, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        layers = []\n",
    "        dense_layers = []\n",
    "        num_levels = len(num_hidden_dim)\n",
    "        for i in range(num_levels):\n",
    "            in_size = input_size if i == 0 else num_hidden_dim[i - 1]\n",
    "            out_size = num_hidden_dim[i]\n",
    "            layers += [RNN_Attn_Block(in_size, out_size, i, dropout=0)]\n",
    "\n",
    "        input_dim = num_hidden_dim[-1]\n",
    "        while (input_dim > 4):\n",
    "            dense_layers += [\n",
    "                nn.Linear(input_dim, round(input_dim / 2)),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            input_dim = round(input_dim / 2)\n",
    "\n",
    "        dense_layers += [nn.Linear(input_dim, 1)]\n",
    "\n",
    "        self.blks = nn.Sequential(*layers)\n",
    "        self.dense = nn.Sequential(*dense_layers)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        '''\n",
    "        x shape (batch_size, 1, input_size)\n",
    "        state[0] cnn_rnn_out shape (batch_size, 72, hidden_dim)\n",
    "        state[1] rnn_cnn_out shape (batch_size, hidden_dim, hidden_dim)\n",
    "        state[2] shape n * (num_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            x, state = blk(x, state)\n",
    "\n",
    "        out = self.dense(x)\n",
    "        #out shape (batch_size, 1, 1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486ff4d",
   "metadata": {},
   "source": [
    "### DARNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "640279da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:38.909122Z",
     "start_time": "2022-01-19T07:44:38.903678Z"
    }
   },
   "outputs": [],
   "source": [
    "class DARNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 num_channels,\n",
    "                 seq_len,\n",
    "                 num_hidden_dim,\n",
    "                 dropout=0.5):\n",
    "        super(DARNet, self).__init__()\n",
    "        # params\n",
    "        self.num_layers = len(num_hidden_dim)\n",
    "        # layers\n",
    "        self.encoder = Encoder(input_size, num_channels, seq_len, dropout)\n",
    "        self.decoder = Decoder(input_size, num_hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "        enc_inputs shape (batch_size, seq_len, input_size)\n",
    "        dec_inputs shape (batch_size, tar_len, input_size)\n",
    "        '''\n",
    "        rho = torch.ones(1, device=device)\n",
    "        y_ = dec_inputs[:, :1, -1].clone()\n",
    "        # y_ shape (batch_size, 1)\n",
    "        dec_inputs[:, :1, -1] = dec_inputs[:, :1, -1] - rho * torch.mean(\n",
    "            dec_inputs[:, :, -1], dim=-1, keepdim=True)\n",
    "\n",
    "        cnn_rnn_out, rnn_cnn_out, hidden_state = self.encoder(enc_inputs, rho)\n",
    "        state = [cnn_rnn_out, rnn_cnn_out, [hidden_state] * self.num_layers]\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(dec_inputs.shape[1]):\n",
    "            if i:\n",
    "                x = torch.cat((dec_inputs[:, i:i + 1, :-1], out.detach()),\n",
    "                              dim=-1)\n",
    "            else:\n",
    "                x = dec_inputs[:, i:i + 1, :]\n",
    "                # x shape (batch_size, 1, input_size)\n",
    "            out, state = self.decoder(x, state)\n",
    "            # out shape (batch_size, 1, 1)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.cat(outputs, dim=1).squeeze(-1)\n",
    "        outputs = outputs + rho * y_\n",
    "        # outputs shape (batch_size, 24)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0caec",
   "metadata": {},
   "source": [
    "### test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bec054b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:42.760088Z",
     "start_time": "2022-01-19T07:44:38.910032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 24])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DARNet(16, [32, 64, 64], 72, [64, 64, 64]).to(device)\n",
    "x_1 = torch.randn(10, 72, 16).to(device)\n",
    "x_2 = torch.randn(10, 24, 16).to(device)\n",
    "out = model(x_1, x_2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe077212",
   "metadata": {},
   "source": [
    "## lr-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce61b016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T08:04:33.227680Z",
     "start_time": "2022-01-16T08:04:33.215548Z"
    }
   },
   "outputs": [],
   "source": [
    "class SchedulerCosineDecayWarmup:\n",
    "    def __init__(self, optimizer, lr, warmup_len, total_iters):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.warmup_len = warmup_len\n",
    "        self.total_iters = total_iters\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.current_iter < self.warmup_len:\n",
    "            lr = self.lr * (self.current_iter + 1) / self.warmup_len\n",
    "        else:\n",
    "            cur = self.current_iter - self.warmup_len\n",
    "            total = self.total_iters - self.warmup_len\n",
    "            lr = 0.1 * (1 + 9 * np.cos(np.pi * cur / total)) * self.lr\n",
    "        return lr\n",
    "\n",
    "    def step(self):\n",
    "        lr = self.get_lr()\n",
    "        for param in self.optimizer.param_groups:\n",
    "            param['lr'] = lr\n",
    "        self.current_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84baa974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:30:19.661769Z",
     "start_time": "2021-12-22T08:30:19.656770Z"
    }
   },
   "source": [
    "## model training for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6753056d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T12:20:31.837025Z",
     "start_time": "2022-01-16T12:20:31.811556Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_model_hpo(train_x_list, train_y_list, valid_x_list, valid_y_list,\n",
    "                    input_size, seq_len, target_len, mse_thresh, hidden_dim,\n",
    "                    n_layers, number_epoch, batch_size, lr, drop_prob,\n",
    "                    weight_decay):\n",
    "    valid_loss_list = []\n",
    "    for num in range(len(train_x_list)):\n",
    "        while (1):\n",
    "            model = Seq2Seq_Attn(input_size, hidden_dim, n_layers, drop_prob)\n",
    "            model = model.to(device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                         lr=lr,\n",
    "                                         weight_decay=weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        1,\n",
    "                                                        gamma=0.98)\n",
    "            valid_loss_min = np.Inf\n",
    "            print('train dataset {}'.format(num))\n",
    "            train_x = train_x_list[num]\n",
    "            train_y = train_y_list[num]\n",
    "            valid_x = valid_x_list[num]\n",
    "            valid_y = valid_y_list[num]\n",
    "            train_dataset = TensorDataset(torch.FloatTensor(train_x),\n",
    "                                          torch.FloatTensor(train_y))\n",
    "            valid_dataset = TensorDataset(torch.FloatTensor(valid_x),\n",
    "                                          torch.FloatTensor(valid_y))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "            valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "            train_losses = list()\n",
    "\n",
    "            num_without_imp = 0\n",
    "\n",
    "            #train\n",
    "            for epoch in range(1, number_epoch + 1):\n",
    "                loop = tqdm(enumerate(train_loader),\n",
    "                            total=len(train_loader),\n",
    "                            leave=True,\n",
    "                            ncols=100)\n",
    "                for i, (inputs, labels) in loop:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    encoder_inputs = inputs\n",
    "                    decoder_inputs = torch.cat(\n",
    "                        (inputs[:, -1:, :], labels[:, :-1, :]), dim=1)\n",
    "                    outputs = model(encoder_inputs, decoder_inputs)\n",
    "                    loss = criterion(outputs, labels[:, :, -1])\n",
    "                    train_losses.append(loss.item)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # eval\n",
    "                    if i % 5 == 0:\n",
    "                        num_without_imp = num_without_imp + 1\n",
    "                        valid_losses = list()\n",
    "                        model.eval()\n",
    "                        for inp, lab in valid_loader:\n",
    "                            inp = inp.to(device)\n",
    "                            lab = lab.to(device)\n",
    "                            encoder_inp = inp\n",
    "                            decoder_inp = torch.cat(\n",
    "                                (inp[:, -1:, :], lab[:, :-1, :]), dim=1)\n",
    "                            out = model(encoder_inp, decoder_inp)\n",
    "                            valid_loss = criterion(out, lab[:, :, -1])\n",
    "                            valid_losses.append(valid_loss.item())\n",
    "                        model.train()\n",
    "                        loop.set_description(\"Epoch: {}/{}...\".format(\n",
    "                            epoch, number_epoch))\n",
    "                        loop.set_postfix(train_loss=loss.item(),\n",
    "                                         valid_loss=np.mean(valid_losses))\n",
    "                        if np.mean(valid_losses) < valid_loss_min:\n",
    "                            num_without_imp = 0\n",
    "                            valid_loss_min = np.mean(valid_losses)\n",
    "                if num_without_imp > 50:\n",
    "                    pass\n",
    "\n",
    "\n",
    "#                     break\n",
    "                scheduler.step()\n",
    "            if valid_loss_min < mse_thresh:\n",
    "                valid_loss_list.append(valid_loss_min)\n",
    "                break\n",
    "    return np.mean(valid_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998735df",
   "metadata": {},
   "source": [
    "## hyper-parameters config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f747e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T07:44:47.970820Z",
     "start_time": "2022-01-19T07:44:47.959681Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = 72\n",
    "target_len = 24\n",
    "mse_thresh = 0.05\n",
    "\n",
    "\n",
    "def model_config():\n",
    "    batch_sizes = [256, 512]\n",
    "    lrs = [0.01]\n",
    "    number_epochs = [40]\n",
    "    hidden_dims = [64, 128]\n",
    "    n_layers = [2, 3]\n",
    "    drop_prob = [0]\n",
    "    weight_decays = [0]\n",
    "    configs = list()\n",
    "    for i in batch_sizes:\n",
    "        for j in lrs:\n",
    "            for k in number_epochs:\n",
    "                for l in hidden_dims:\n",
    "                    for m in n_layers:\n",
    "                        for n in drop_prob:\n",
    "                            for o in weight_decays:\n",
    "                                configs.append({\n",
    "                                    'batch_size': i,\n",
    "                                    'lr': j,\n",
    "                                    'number_epoch': k,\n",
    "                                    'hidden_dim': l,\n",
    "                                    'n_layers': m,\n",
    "                                    'drop_prob': n,\n",
    "                                    'weight_decay': o\n",
    "                                })\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073677ff",
   "metadata": {},
   "source": [
    "## random search for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25244a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T07:42:23.548367Z",
     "start_time": "2022-01-09T07:42:23.542211Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_hpo(seq_len=seq_len,\n",
    "                  target_len=target_len,\n",
    "                  mse_thresh=mse_thresh):\n",
    "    train_data = data[:int(0.8 * len(data))]\n",
    "    train_data, _, _ = normalization(train_data)\n",
    "    train_x, train_y = series_to_supervise(train_data, seq_len, target_len)\n",
    "    train_x_list, train_y_list, valid_x_list, valid_y_list = time_series_split(\n",
    "        train_x, train_y)\n",
    "    #         with enough data\n",
    "    train_x_list = train_x_list[-1:]\n",
    "    train_y_list = train_y_list[-1:]\n",
    "    valid_x_list = valid_x_list[-1:]\n",
    "    valid_y_list = valid_y_list[-1:]\n",
    "\n",
    "    configs = model_config()\n",
    "    records = []\n",
    "    input_size = train_x.shape[2]\n",
    "    for i in range(6):\n",
    "        config = random.choice(configs)\n",
    "        configs.remove(config)\n",
    "        batch_size = config['batch_size']\n",
    "        lr = config['lr']\n",
    "        number_epoch = config['number_epoch']\n",
    "        hidden_dim = config['hidden_dim']\n",
    "        n_layers = config['n_layers']\n",
    "        drop_prob = config['drop_prob']\n",
    "        weight_decay = config['weight_decay']\n",
    "        print(\n",
    "            \"model config: batch_size-{}, lr-{}, number_epoch-{}, hidden_dim-{}, n_layers-{},drop_prob-{},weight_decay-{}\"\n",
    "            .format(batch_size, lr, number_epoch, hidden_dim, n_layers,\n",
    "                    drop_prob, weight_decay))\n",
    "        valid_loss = train_model_hpo(\n",
    "            train_x_list,\n",
    "            train_y_list,\n",
    "            valid_x_list,\n",
    "            valid_y_list,\n",
    "            input_size,\n",
    "            seq_len,\n",
    "            target_len,\n",
    "            mse_thresh,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            number_epoch,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            drop_prob,\n",
    "            weight_decay,\n",
    "        )\n",
    "        records.append({\n",
    "            'batch_size': batch_size,\n",
    "            'lr': lr,\n",
    "            'number_epoch': number_epoch,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'n_layers': n_layers,\n",
    "            'drop_prob': drop_prob,\n",
    "            'weight_decay': weight_decay,\n",
    "            'valid_loss': valid_loss\n",
    "        })\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbd3cc",
   "metadata": {},
   "source": [
    "## RUN random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4503a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T09:35:59.754572Z",
     "start_time": "2022-01-05T09:07:18.790644Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised data: shape of x: (25232, 72, 16), shape of y: (25232, 24, 16)\n",
      "model config: batch_size-512, lr-0.01, number_epoch-40, hidden_dim-128, n_layers-3,drop_prob-0,weight_decay-0\n",
      "train dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...: 100%|█████████| 41/41 [00:04<00:00,  9.67it/s, train_loss=0.0219, valid_loss=0.0426]\n",
      "Epoch: 2/40...: 100%|█████████| 41/41 [00:03<00:00, 10.28it/s, train_loss=0.0239, valid_loss=0.0417]\n",
      "Epoch: 3/40...: 100%|██████████| 41/41 [00:04<00:00,  9.91it/s, train_loss=0.014, valid_loss=0.0243]\n",
      "Epoch: 4/40...: 100%|████████████| 41/41 [00:04<00:00,  9.99it/s, train_loss=0.011, valid_loss=0.02]\n",
      "Epoch: 5/40...: 100%|█████████| 41/41 [00:04<00:00,  9.92it/s, train_loss=0.00944, valid_loss=0.016]\n",
      "Epoch: 6/40...: 100%|████████| 41/41 [00:04<00:00,  9.83it/s, train_loss=0.00826, valid_loss=0.0153]\n",
      "Epoch: 7/40...: 100%|███████| 41/41 [00:04<00:00,  9.88it/s, train_loss=0.00361, valid_loss=0.00528]\n",
      "Epoch: 8/40...: 100%|███████| 41/41 [00:04<00:00, 10.21it/s, train_loss=0.00285, valid_loss=0.00387]\n",
      "Epoch: 9/40...: 100%|█████████| 41/41 [00:04<00:00, 10.09it/s, train_loss=0.00207, valid_loss=0.003]\n",
      "Epoch: 10/40...: 100%|██████| 41/41 [00:04<00:00,  9.70it/s, train_loss=0.00198, valid_loss=0.00361]\n",
      "Epoch: 11/40...: 100%|██████| 41/41 [00:04<00:00, 10.09it/s, train_loss=0.00213, valid_loss=0.00362]\n",
      "Epoch: 12/40...: 100%|██████| 41/41 [00:04<00:00,  9.75it/s, train_loss=0.00169, valid_loss=0.00322]\n",
      "Epoch: 13/40...: 100%|██████| 41/41 [00:04<00:00,  9.85it/s, train_loss=0.00189, valid_loss=0.00235]\n",
      "Epoch: 14/40...: 100%|██████| 41/41 [00:04<00:00, 10.00it/s, train_loss=0.00173, valid_loss=0.00247]\n",
      "Epoch: 15/40...: 100%|██████| 41/41 [00:04<00:00,  9.97it/s, train_loss=0.00173, valid_loss=0.00329]\n",
      "Epoch: 16/40...: 100%|██████| 41/41 [00:04<00:00, 10.25it/s, train_loss=0.00125, valid_loss=0.00256]\n",
      "Epoch: 17/40...: 100%|███████| 41/41 [00:04<00:00, 10.10it/s, train_loss=0.0018, valid_loss=0.00227]\n",
      "Epoch: 18/40...: 100%|██████| 41/41 [00:04<00:00, 10.08it/s, train_loss=0.00136, valid_loss=0.00231]\n",
      "Epoch: 19/40...: 100%|██████| 41/41 [00:04<00:00, 10.01it/s, train_loss=0.00151, valid_loss=0.00264]\n",
      "Epoch: 20/40...: 100%|██████| 41/41 [00:04<00:00,  9.96it/s, train_loss=0.00194, valid_loss=0.00313]\n",
      "Epoch: 21/40...: 100%|██████| 41/41 [00:04<00:00, 10.08it/s, train_loss=0.00112, valid_loss=0.00201]\n",
      "Epoch: 22/40...: 100%|█████| 41/41 [00:04<00:00,  9.93it/s, train_loss=0.000903, valid_loss=0.00172]\n",
      "Epoch: 23/40...: 100%|██████| 41/41 [00:04<00:00, 10.24it/s, train_loss=0.00109, valid_loss=0.00179]\n",
      "Epoch: 24/40...: 100%|█████| 41/41 [00:04<00:00,  9.80it/s, train_loss=0.000911, valid_loss=0.00174]\n",
      "Epoch: 25/40...: 100%|██████| 41/41 [00:04<00:00,  9.68it/s, train_loss=0.000788, valid_loss=0.0017]\n",
      "Epoch: 26/40...: 100%|█████| 41/41 [00:04<00:00, 10.12it/s, train_loss=0.000992, valid_loss=0.00183]\n",
      "Epoch: 27/40...: 100%|█████| 41/41 [00:04<00:00,  9.79it/s, train_loss=0.000748, valid_loss=0.00132]\n",
      "Epoch: 28/40...: 100%|█████| 41/41 [00:04<00:00, 10.09it/s, train_loss=0.000996, valid_loss=0.00145]\n",
      "Epoch: 29/40...: 100%|█████| 41/41 [00:04<00:00,  9.99it/s, train_loss=0.000765, valid_loss=0.00165]\n",
      "Epoch: 30/40...: 100%|██████| 41/41 [00:04<00:00,  9.56it/s, train_loss=0.00129, valid_loss=0.00326]\n",
      "Epoch: 31/40...: 100%|█████| 41/41 [00:04<00:00,  9.49it/s, train_loss=0.000535, valid_loss=0.00132]\n",
      "Epoch: 32/40...: 100%|█████| 41/41 [00:04<00:00,  9.47it/s, train_loss=0.000547, valid_loss=0.00114]\n",
      "Epoch: 33/40...: 100%|████| 41/41 [00:04<00:00,  9.75it/s, train_loss=0.000459, valid_loss=0.000865]\n",
      "Epoch: 34/40...: 100%|█████| 41/41 [00:04<00:00,  9.65it/s, train_loss=0.000544, valid_loss=0.00102]\n",
      "Epoch: 35/40...: 100%|█████| 41/41 [00:04<00:00,  9.55it/s, train_loss=0.00041, valid_loss=0.000807]\n",
      "Epoch: 36/40...: 100%|█████| 41/41 [00:04<00:00,  9.60it/s, train_loss=0.000426, valid_loss=0.00107]\n",
      "Epoch: 37/40...: 100%|████| 41/41 [00:04<00:00,  9.82it/s, train_loss=0.000364, valid_loss=0.000823]\n",
      "Epoch: 38/40...: 100%|█████| 41/41 [00:04<00:00,  9.90it/s, train_loss=0.000629, valid_loss=0.00124]\n",
      "Epoch: 39/40...: 100%|████| 41/41 [00:04<00:00, 10.05it/s, train_loss=0.000395, valid_loss=0.000808]\n",
      "Epoch: 40/40...: 100%|████| 41/41 [00:04<00:00, 10.06it/s, train_loss=0.000378, valid_loss=0.000778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-40, hidden_dim-64, n_layers-2,drop_prob-0,weight_decay-0\n",
      "train dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...: 100%|███████| 82/82 [00:08<00:00,  9.62it/s, train_loss=0.00376, valid_loss=0.00473]\n",
      "Epoch: 2/40...: 100%|███████| 82/82 [00:08<00:00,  9.47it/s, train_loss=0.00148, valid_loss=0.00249]\n",
      "Epoch: 3/40...: 100%|███████| 82/82 [00:08<00:00,  9.60it/s, train_loss=0.00145, valid_loss=0.00198]\n",
      "Epoch: 4/40...: 100%|███████| 82/82 [00:08<00:00,  9.64it/s, train_loss=0.00106, valid_loss=0.00167]\n",
      "Epoch: 5/40...: 100%|██████| 82/82 [00:08<00:00,  9.72it/s, train_loss=0.000806, valid_loss=0.00139]\n",
      "Epoch: 6/40...: 100%|███████| 82/82 [00:08<00:00,  9.60it/s, train_loss=0.00063, valid_loss=0.00121]\n",
      "Epoch: 7/40...: 100%|█████| 82/82 [00:08<00:00,  9.63it/s, train_loss=0.000475, valid_loss=0.000731]\n",
      "Epoch: 8/40...: 100%|█████| 82/82 [00:08<00:00,  9.68it/s, train_loss=0.000531, valid_loss=0.000657]\n",
      "Epoch: 9/40...: 100%|█████| 82/82 [00:08<00:00,  9.53it/s, train_loss=0.000314, valid_loss=0.000552]\n",
      "Epoch: 10/40...: 100%|████| 82/82 [00:08<00:00,  9.48it/s, train_loss=0.000294, valid_loss=0.000577]\n",
      "Epoch: 11/40...: 100%|████| 82/82 [00:08<00:00,  9.43it/s, train_loss=0.000337, valid_loss=0.000495]\n",
      "Epoch: 12/40...: 100%|████| 82/82 [00:08<00:00,  9.57it/s, train_loss=0.000275, valid_loss=0.000488]\n",
      "Epoch: 13/40...: 100%|████| 82/82 [00:08<00:00,  9.56it/s, train_loss=0.000243, valid_loss=0.000442]\n",
      "Epoch: 14/40...: 100%|████| 82/82 [00:08<00:00,  9.48it/s, train_loss=0.000332, valid_loss=0.000513]\n",
      "Epoch: 15/40...: 100%|████| 82/82 [00:08<00:00,  9.64it/s, train_loss=0.000222, valid_loss=0.000502]\n",
      "Epoch: 16/40...: 100%|████| 82/82 [00:08<00:00, 10.03it/s, train_loss=0.000215, valid_loss=0.000447]\n",
      "Epoch: 17/40...: 100%|████| 82/82 [00:08<00:00,  9.96it/s, train_loss=0.000225, valid_loss=0.000449]\n",
      "Epoch: 18/40...: 100%|████| 82/82 [00:08<00:00, 10.16it/s, train_loss=0.000232, valid_loss=0.000435]\n",
      "Epoch: 19/40...: 100%|████| 82/82 [00:08<00:00,  9.98it/s, train_loss=0.000253, valid_loss=0.000414]\n",
      "Epoch: 20/40...: 100%|████| 82/82 [00:08<00:00, 10.19it/s, train_loss=0.000176, valid_loss=0.000469]\n",
      "Epoch: 21/40...: 100%|████| 82/82 [00:08<00:00, 10.04it/s, train_loss=0.000238, valid_loss=0.000428]\n",
      "Epoch: 22/40...: 100%|████| 82/82 [00:08<00:00, 10.01it/s, train_loss=0.000219, valid_loss=0.000435]\n",
      "Epoch: 23/40...: 100%|████| 82/82 [00:07<00:00, 10.47it/s, train_loss=0.000182, valid_loss=0.000482]\n",
      "Epoch: 24/40...: 100%|████| 82/82 [00:07<00:00, 10.47it/s, train_loss=0.000191, valid_loss=0.000452]\n",
      "Epoch: 25/40...: 100%|████| 82/82 [00:07<00:00, 10.41it/s, train_loss=0.000169, valid_loss=0.000417]\n",
      "Epoch: 26/40...: 100%|████| 82/82 [00:08<00:00, 10.17it/s, train_loss=0.000193, valid_loss=0.000542]\n",
      "Epoch: 27/40...: 100%|████| 82/82 [00:07<00:00, 10.34it/s, train_loss=0.000165, valid_loss=0.000464]\n",
      "Epoch: 28/40...: 100%|████| 82/82 [00:07<00:00, 10.36it/s, train_loss=0.000165, valid_loss=0.000427]\n",
      "Epoch: 29/40...: 100%|████| 82/82 [00:07<00:00, 10.36it/s, train_loss=0.000135, valid_loss=0.000451]\n",
      "Epoch: 30/40...: 100%|█████| 82/82 [00:07<00:00, 10.43it/s, train_loss=0.00014, valid_loss=0.000416]\n",
      "Epoch: 31/40...: 100%|████| 82/82 [00:07<00:00, 10.27it/s, train_loss=0.000194, valid_loss=0.000458]\n",
      "Epoch: 32/40...: 100%|████| 82/82 [00:07<00:00, 10.34it/s, train_loss=0.000127, valid_loss=0.000475]\n",
      "Epoch: 33/40...: 100%|█████| 82/82 [00:07<00:00, 10.44it/s, train_loss=0.00019, valid_loss=0.000436]\n",
      "Epoch: 34/40...: 100%|████| 82/82 [00:07<00:00, 10.51it/s, train_loss=0.000134, valid_loss=0.000484]\n",
      "Epoch: 35/40...: 100%|█████| 82/82 [00:07<00:00, 10.34it/s, train_loss=0.000153, valid_loss=0.00044]\n",
      "Epoch: 36/40...: 100%|████| 82/82 [00:07<00:00, 10.56it/s, train_loss=0.000168, valid_loss=0.000483]\n",
      "Epoch: 37/40...: 100%|████| 82/82 [00:07<00:00, 10.48it/s, train_loss=0.000146, valid_loss=0.000514]\n",
      "Epoch: 38/40...: 100%|████| 82/82 [00:08<00:00, 10.24it/s, train_loss=0.000115, valid_loss=0.000477]\n",
      "Epoch: 39/40...: 100%|████| 82/82 [00:07<00:00, 10.54it/s, train_loss=0.000127, valid_loss=0.000429]\n",
      "Epoch: 40/40...: 100%|████| 82/82 [00:07<00:00, 10.32it/s, train_loss=0.000111, valid_loss=0.000448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-512, lr-0.01, number_epoch-40, hidden_dim-64, n_layers-2,drop_prob-0,weight_decay-0\n",
      "train dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...: 100%|█████████| 41/41 [00:03<00:00, 12.91it/s, train_loss=0.0227, valid_loss=0.0425]\n",
      "Epoch: 2/40...: 100%|█████████| 41/41 [00:03<00:00, 12.64it/s, train_loss=0.0239, valid_loss=0.0426]\n",
      "Epoch: 3/40...: 100%|█████████| 41/41 [00:03<00:00, 13.07it/s, train_loss=0.0237, valid_loss=0.0426]\n",
      "Epoch: 4/40...: 100%|█████████| 41/41 [00:03<00:00, 12.63it/s, train_loss=0.0243, valid_loss=0.0427]\n",
      "Epoch: 5/40...: 100%|█████████| 41/41 [00:03<00:00, 12.47it/s, train_loss=0.0247, valid_loss=0.0426]\n",
      "Epoch: 6/40...: 100%|█████████| 41/41 [00:03<00:00, 12.84it/s, train_loss=0.0223, valid_loss=0.0426]\n",
      "Epoch: 7/40...: 100%|█████████| 41/41 [00:03<00:00, 12.91it/s, train_loss=0.0239, valid_loss=0.0427]\n",
      "Epoch: 8/40...: 100%|█████████| 41/41 [00:03<00:00, 12.86it/s, train_loss=0.0226, valid_loss=0.0426]\n",
      "Epoch: 9/40...: 100%|█████████| 41/41 [00:03<00:00, 12.29it/s, train_loss=0.0253, valid_loss=0.0425]\n",
      "Epoch: 10/40...: 100%|████████| 41/41 [00:03<00:00, 12.60it/s, train_loss=0.0238, valid_loss=0.0426]\n",
      "Epoch: 11/40...: 100%|████████| 41/41 [00:03<00:00, 12.90it/s, train_loss=0.0237, valid_loss=0.0425]\n",
      "Epoch: 12/40...: 100%|████████| 41/41 [00:03<00:00, 12.95it/s, train_loss=0.0239, valid_loss=0.0424]\n",
      "Epoch: 13/40...: 100%|████████| 41/41 [00:03<00:00, 12.48it/s, train_loss=0.0257, valid_loss=0.0427]\n",
      "Epoch: 14/40...: 100%|████████| 41/41 [00:03<00:00, 12.54it/s, train_loss=0.0243, valid_loss=0.0425]\n",
      "Epoch: 15/40...: 100%|████████| 41/41 [00:03<00:00, 12.71it/s, train_loss=0.0253, valid_loss=0.0425]\n",
      "Epoch: 16/40...: 100%|████████| 41/41 [00:03<00:00, 12.66it/s, train_loss=0.0232, valid_loss=0.0427]\n",
      "Epoch: 17/40...: 100%|████████| 41/41 [00:03<00:00, 12.65it/s, train_loss=0.0243, valid_loss=0.0426]\n",
      "Epoch: 18/40...: 100%|████████| 41/41 [00:03<00:00, 12.75it/s, train_loss=0.0232, valid_loss=0.0426]\n",
      "Epoch: 19/40...: 100%|████████| 41/41 [00:03<00:00, 12.31it/s, train_loss=0.0244, valid_loss=0.0426]\n",
      "Epoch: 20/40...: 100%|████████| 41/41 [00:03<00:00, 12.86it/s, train_loss=0.0238, valid_loss=0.0424]\n",
      "Epoch: 21/40...: 100%|████████| 41/41 [00:03<00:00, 12.98it/s, train_loss=0.0247, valid_loss=0.0424]\n",
      "Epoch: 22/40...: 100%|████████| 41/41 [00:03<00:00, 12.33it/s, train_loss=0.0235, valid_loss=0.0425]\n",
      "Epoch: 23/40...: 100%|████████| 41/41 [00:03<00:00, 12.53it/s, train_loss=0.0231, valid_loss=0.0426]\n",
      "Epoch: 24/40...: 100%|████████| 41/41 [00:03<00:00, 12.84it/s, train_loss=0.0235, valid_loss=0.0425]\n",
      "Epoch: 25/40...: 100%|████████| 41/41 [00:03<00:00, 12.47it/s, train_loss=0.0242, valid_loss=0.0426]\n",
      "Epoch: 26/40...: 100%|████████| 41/41 [00:03<00:00, 12.97it/s, train_loss=0.0238, valid_loss=0.0425]\n",
      "Epoch: 27/40...: 100%|████████| 41/41 [00:03<00:00, 12.50it/s, train_loss=0.0226, valid_loss=0.0427]\n",
      "Epoch: 28/40...: 100%|████████| 41/41 [00:03<00:00, 12.76it/s, train_loss=0.0241, valid_loss=0.0424]\n",
      "Epoch: 29/40...: 100%|████████| 41/41 [00:03<00:00, 12.43it/s, train_loss=0.0239, valid_loss=0.0425]\n",
      "Epoch: 30/40...: 100%|████████| 41/41 [00:03<00:00, 12.65it/s, train_loss=0.0244, valid_loss=0.0427]\n",
      "Epoch: 31/40...: 100%|█████████| 41/41 [00:03<00:00, 12.30it/s, train_loss=0.023, valid_loss=0.0425]\n",
      "Epoch: 32/40...: 100%|████████| 41/41 [00:03<00:00, 12.34it/s, train_loss=0.0242, valid_loss=0.0424]\n",
      "Epoch: 33/40...: 100%|████████| 41/41 [00:03<00:00, 12.84it/s, train_loss=0.0232, valid_loss=0.0426]\n",
      "Epoch: 34/40...: 100%|████████| 41/41 [00:03<00:00, 12.76it/s, train_loss=0.0236, valid_loss=0.0424]\n",
      "Epoch: 35/40...: 100%|████████| 41/41 [00:03<00:00, 12.51it/s, train_loss=0.0232, valid_loss=0.0427]\n",
      "Epoch: 36/40...: 100%|████████| 41/41 [00:03<00:00, 12.43it/s, train_loss=0.0228, valid_loss=0.0426]\n",
      "Epoch: 37/40...: 100%|████████| 41/41 [00:03<00:00, 12.24it/s, train_loss=0.0234, valid_loss=0.0426]\n",
      "Epoch: 38/40...: 100%|████████| 41/41 [00:03<00:00, 12.51it/s, train_loss=0.0231, valid_loss=0.0426]\n",
      "Epoch: 39/40...: 100%|████████| 41/41 [00:03<00:00, 12.43it/s, train_loss=0.0248, valid_loss=0.0426]\n",
      "Epoch: 40/40...: 100%|████████| 41/41 [00:03<00:00, 12.79it/s, train_loss=0.0233, valid_loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-40, hidden_dim-128, n_layers-3,drop_prob-0,weight_decay-0\n",
      "train dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...: 100%|█████████| 82/82 [00:09<00:00,  8.63it/s, train_loss=0.0247, valid_loss=0.0427]\n",
      "Epoch: 2/40...: 100%|█████████| 82/82 [00:09<00:00,  8.52it/s, train_loss=0.0131, valid_loss=0.0219]\n",
      "Epoch: 3/40...: 100%|█████████| 82/82 [00:09<00:00,  8.65it/s, train_loss=0.0179, valid_loss=0.0307]\n",
      "Epoch: 4/40...: 100%|█████████| 82/82 [00:09<00:00,  8.65it/s, train_loss=0.0133, valid_loss=0.0223]\n",
      "Epoch: 5/40...: 100%|██████████| 82/82 [00:09<00:00,  8.63it/s, train_loss=0.0102, valid_loss=0.019]\n",
      "Epoch: 6/40...: 100%|████████| 82/82 [00:09<00:00,  8.70it/s, train_loss=0.0046, valid_loss=0.00762]\n",
      "Epoch: 7/40...: 100%|███████| 82/82 [00:09<00:00,  8.73it/s, train_loss=0.00328, valid_loss=0.00444]\n",
      "Epoch: 8/40...: 100%|███████| 82/82 [00:09<00:00,  8.62it/s, train_loss=0.00264, valid_loss=0.00357]\n",
      "Epoch: 9/40...: 100%|███████| 82/82 [00:09<00:00,  8.67it/s, train_loss=0.00211, valid_loss=0.00351]\n",
      "Epoch: 10/40...: 100%|██████| 82/82 [00:09<00:00,  8.61it/s, train_loss=0.00325, valid_loss=0.00434]\n",
      "Epoch: 11/40...: 100%|██████| 82/82 [00:09<00:00,  8.64it/s, train_loss=0.00201, valid_loss=0.00291]\n",
      "Epoch: 12/40...: 100%|██████| 82/82 [00:09<00:00,  8.60it/s, train_loss=0.00181, valid_loss=0.00289]\n",
      "Epoch: 13/40...: 100%|███████| 82/82 [00:09<00:00,  8.64it/s, train_loss=0.00178, valid_loss=0.0024]\n",
      "Epoch: 14/40...: 100%|██████| 82/82 [00:09<00:00,  8.58it/s, train_loss=0.00168, valid_loss=0.00275]\n",
      "Epoch: 15/40...: 100%|██████| 82/82 [00:09<00:00,  8.58it/s, train_loss=0.00161, valid_loss=0.00275]\n",
      "Epoch: 16/40...: 100%|██████| 82/82 [00:09<00:00,  8.59it/s, train_loss=0.00192, valid_loss=0.00303]\n",
      "Epoch: 17/40...: 100%|██████| 82/82 [00:09<00:00,  8.47it/s, train_loss=0.00176, valid_loss=0.00319]\n",
      "Epoch: 18/40...: 100%|██████| 82/82 [00:09<00:00,  8.59it/s, train_loss=0.00153, valid_loss=0.00289]\n",
      "Epoch: 19/40...: 100%|██████| 82/82 [00:09<00:00,  8.57it/s, train_loss=0.00191, valid_loss=0.00261]\n",
      "Epoch: 20/40...: 100%|███████| 82/82 [00:09<00:00,  8.72it/s, train_loss=0.00131, valid_loss=0.0027]\n",
      "Epoch: 21/40...: 100%|██████| 82/82 [00:09<00:00,  8.57it/s, train_loss=0.00144, valid_loss=0.00215]\n",
      "Epoch: 22/40...: 100%|██████| 82/82 [00:09<00:00,  8.56it/s, train_loss=0.00151, valid_loss=0.00279]\n",
      "Epoch: 23/40...: 100%|███████| 82/82 [00:09<00:00,  8.64it/s, train_loss=0.0015, valid_loss=0.00263]\n",
      "Epoch: 24/40...: 100%|███████| 82/82 [00:09<00:00,  8.56it/s, train_loss=0.00109, valid_loss=0.0023]\n",
      "Epoch: 25/40...: 100%|█████| 82/82 [00:09<00:00,  8.67it/s, train_loss=0.000889, valid_loss=0.00183]\n",
      "Epoch: 26/40...: 100%|██████| 82/82 [00:09<00:00,  8.52it/s, train_loss=0.00116, valid_loss=0.00185]\n",
      "Epoch: 27/40...: 100%|██████| 82/82 [00:09<00:00,  8.45it/s, train_loss=0.00113, valid_loss=0.00128]\n",
      "Epoch: 28/40...: 100%|█████| 82/82 [00:09<00:00,  8.51it/s, train_loss=0.000798, valid_loss=0.00126]\n",
      "Epoch: 29/40...: 100%|█████| 82/82 [00:09<00:00,  8.55it/s, train_loss=0.000697, valid_loss=0.00107]\n",
      "Epoch: 30/40...: 100%|████| 82/82 [00:09<00:00,  8.52it/s, train_loss=0.000629, valid_loss=0.000881]\n",
      "Epoch: 31/40...: 100%|████| 82/82 [00:09<00:00,  8.60it/s, train_loss=0.000681, valid_loss=0.000969]\n",
      "Epoch: 32/40...: 100%|████| 82/82 [00:09<00:00,  8.52it/s, train_loss=0.000547, valid_loss=0.000911]\n",
      "Epoch: 33/40...: 100%|████| 82/82 [00:09<00:00,  8.58it/s, train_loss=0.000461, valid_loss=0.000862]\n",
      "Epoch: 34/40...: 100%|████| 82/82 [00:09<00:00,  8.53it/s, train_loss=0.000389, valid_loss=0.000816]\n",
      "Epoch: 35/40...: 100%|█████| 82/82 [00:09<00:00,  8.37it/s, train_loss=0.00046, valid_loss=0.000693]\n",
      "Epoch: 36/40...: 100%|████| 82/82 [00:09<00:00,  8.51it/s, train_loss=0.000393, valid_loss=0.000755]\n",
      "Epoch: 37/40...: 100%|████| 82/82 [00:09<00:00,  8.51it/s, train_loss=0.000422, valid_loss=0.000649]\n",
      "Epoch: 38/40...: 100%|████| 82/82 [00:09<00:00,  8.64it/s, train_loss=0.000357, valid_loss=0.000687]\n",
      "Epoch: 39/40...: 100%|████| 82/82 [00:09<00:00,  8.62it/s, train_loss=0.000407, valid_loss=0.000688]\n",
      "Epoch: 40/40...: 100%|████| 82/82 [00:09<00:00,  8.58it/s, train_loss=0.000351, valid_loss=0.000621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-40, hidden_dim-128, n_layers-2,drop_prob-0,weight_decay-0\n",
      "train dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...: 100%|█████████| 82/82 [00:09<00:00,  9.10it/s, train_loss=0.0151, valid_loss=0.0281]\n",
      "Epoch: 2/40...: 100%|████████| 82/82 [00:08<00:00,  9.14it/s, train_loss=0.00722, valid_loss=0.0132]\n",
      "Epoch: 3/40...: 100%|███████| 82/82 [00:09<00:00,  9.04it/s, train_loss=0.00217, valid_loss=0.00312]\n",
      "Epoch: 4/40...: 100%|███████| 82/82 [00:08<00:00,  9.11it/s, train_loss=0.00155, valid_loss=0.00309]\n",
      "Epoch: 5/40...: 100%|███████| 82/82 [00:08<00:00,  9.14it/s, train_loss=0.00124, valid_loss=0.00283]\n",
      "Epoch: 6/40...: 100%|███████| 82/82 [00:08<00:00,  9.12it/s, train_loss=0.00107, valid_loss=0.00213]\n",
      "Epoch: 7/40...: 100%|██████| 82/82 [00:08<00:00,  9.18it/s, train_loss=0.000996, valid_loss=0.00208]\n",
      "Epoch: 8/40...: 100%|██████| 82/82 [00:09<00:00,  9.05it/s, train_loss=0.000968, valid_loss=0.00211]\n",
      "Epoch: 9/40...: 100%|████████| 82/82 [00:08<00:00,  9.13it/s, train_loss=0.000708, valid_loss=0.002]\n",
      "Epoch: 10/40...: 100%|█████| 82/82 [00:09<00:00,  9.00it/s, train_loss=0.000664, valid_loss=0.00231]\n",
      "Epoch: 11/40...: 100%|█████| 82/82 [00:09<00:00,  9.05it/s, train_loss=0.000633, valid_loss=0.00211]\n",
      "Epoch: 12/40...: 100%|█████| 82/82 [00:09<00:00,  9.06it/s, train_loss=0.000587, valid_loss=0.00236]\n",
      "Epoch: 13/40...: 100%|██████| 82/82 [00:09<00:00,  8.99it/s, train_loss=0.000454, valid_loss=0.0022]\n",
      "Epoch: 14/40...: 100%|██████| 82/82 [00:09<00:00,  9.01it/s, train_loss=0.00044, valid_loss=0.00202]\n",
      "Epoch: 15/40...: 100%|██████| 82/82 [00:09<00:00,  9.04it/s, train_loss=0.00032, valid_loss=0.00206]\n",
      "Epoch: 16/40...: 100%|█████| 82/82 [00:09<00:00,  9.04it/s, train_loss=0.000414, valid_loss=0.00207]\n",
      "Epoch: 17/40...: 100%|█████| 82/82 [00:08<00:00,  9.17it/s, train_loss=0.000299, valid_loss=0.00229]\n",
      "Epoch: 18/40...: 100%|█████| 82/82 [00:08<00:00,  9.14it/s, train_loss=0.000296, valid_loss=0.00227]\n",
      "Epoch: 19/40...: 100%|█████| 82/82 [00:09<00:00,  9.02it/s, train_loss=0.000271, valid_loss=0.00218]\n",
      "Epoch: 20/40...: 100%|█████| 82/82 [00:08<00:00,  9.13it/s, train_loss=0.000236, valid_loss=0.00203]\n",
      "Epoch: 21/40...: 100%|█████| 82/82 [00:09<00:00,  9.08it/s, train_loss=0.000208, valid_loss=0.00212]\n",
      "Epoch: 22/40...: 100%|█████| 82/82 [00:09<00:00,  9.10it/s, train_loss=0.000281, valid_loss=0.00199]\n",
      "Epoch: 23/40...: 100%|█████| 82/82 [00:09<00:00,  8.97it/s, train_loss=0.000229, valid_loss=0.00216]\n",
      "Epoch: 24/40...: 100%|█████| 82/82 [00:09<00:00,  8.98it/s, train_loss=0.000188, valid_loss=0.00197]\n",
      "Epoch: 25/40...: 100%|█████| 82/82 [00:09<00:00,  9.04it/s, train_loss=0.000211, valid_loss=0.00215]\n",
      "Epoch: 26/40...: 100%|█████| 82/82 [00:08<00:00,  9.41it/s, train_loss=0.000193, valid_loss=0.00216]\n",
      "Epoch: 27/40...: 100%|█████| 82/82 [00:08<00:00,  9.36it/s, train_loss=0.000193, valid_loss=0.00219]\n",
      "Epoch: 28/40...: 100%|█████| 82/82 [00:08<00:00,  9.32it/s, train_loss=0.000191, valid_loss=0.00209]\n",
      "Epoch: 29/40...: 100%|█████| 82/82 [00:08<00:00,  9.31it/s, train_loss=0.000226, valid_loss=0.00214]\n",
      "Epoch: 30/40...: 100%|█████| 82/82 [00:08<00:00,  9.28it/s, train_loss=0.000276, valid_loss=0.00232]\n",
      "Epoch: 31/40...: 100%|█████| 82/82 [00:08<00:00,  9.30it/s, train_loss=0.000166, valid_loss=0.00217]\n",
      "Epoch: 32/40...: 100%|█████| 82/82 [00:08<00:00,  9.41it/s, train_loss=0.000164, valid_loss=0.00204]\n",
      "Epoch: 33/40...: 100%|█████| 82/82 [00:08<00:00,  9.19it/s, train_loss=0.000148, valid_loss=0.00194]\n",
      "Epoch: 34/40...: 100%|██████| 82/82 [00:08<00:00,  9.31it/s, train_loss=0.00015, valid_loss=0.00209]\n",
      "Epoch: 35/40...: 100%|██████| 82/82 [00:08<00:00,  9.27it/s, train_loss=0.000139, valid_loss=0.0021]\n",
      "Epoch: 36/40...: 100%|█████| 82/82 [00:08<00:00,  9.22it/s, train_loss=0.000182, valid_loss=0.00224]\n",
      "Epoch: 37/40...: 100%|██████| 82/82 [00:09<00:00,  9.08it/s, train_loss=0.00012, valid_loss=0.00208]\n",
      "Epoch: 38/40...: 100%|█████| 82/82 [00:08<00:00,  9.18it/s, train_loss=0.000112, valid_loss=0.00207]\n",
      "Epoch: 39/40...: 100%|█████| 82/82 [00:09<00:00,  9.10it/s, train_loss=0.000133, valid_loss=0.00201]\n",
      "Epoch: 40/40...: 100%|█████| 82/82 [00:08<00:00,  9.22it/s, train_loss=0.000127, valid_loss=0.00206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-40, hidden_dim-64, n_layers-3,drop_prob-0,weight_decay-0\n",
      "train dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...: 100%|█████████| 82/82 [00:08<00:00,  9.38it/s, train_loss=0.0101, valid_loss=0.0123]\n",
      "Epoch: 2/40...: 100%|████████| 82/82 [00:08<00:00,  9.42it/s, train_loss=0.0024, valid_loss=0.00311]\n",
      "Epoch: 3/40...: 100%|███████| 82/82 [00:08<00:00,  9.39it/s, train_loss=0.00167, valid_loss=0.00319]\n",
      "Epoch: 4/40...: 100%|████████| 82/82 [00:08<00:00,  9.34it/s, train_loss=0.0012, valid_loss=0.00211]\n",
      "Epoch: 5/40...: 100%|███████| 82/82 [00:08<00:00,  9.39it/s, train_loss=0.00108, valid_loss=0.00185]\n",
      "Epoch: 6/40...: 100%|███████| 82/82 [00:08<00:00,  9.35it/s, train_loss=0.00127, valid_loss=0.00216]\n",
      "Epoch: 7/40...: 100%|██████| 82/82 [00:08<00:00,  9.44it/s, train_loss=0.000893, valid_loss=0.00157]\n",
      "Epoch: 8/40...: 100%|██████| 82/82 [00:08<00:00,  9.34it/s, train_loss=0.000758, valid_loss=0.00105]\n",
      "Epoch: 9/40...: 100%|██████| 82/82 [00:08<00:00,  9.39it/s, train_loss=0.000665, valid_loss=0.00116]\n",
      "Epoch: 10/40...: 100%|█████| 82/82 [00:08<00:00,  9.28it/s, train_loss=0.000498, valid_loss=0.00115]\n",
      "Epoch: 11/40...: 100%|████| 82/82 [00:08<00:00,  9.31it/s, train_loss=0.000449, valid_loss=0.000673]\n",
      "Epoch: 12/40...: 100%|████| 82/82 [00:08<00:00,  9.38it/s, train_loss=0.000588, valid_loss=0.000755]\n",
      "Epoch: 13/40...: 100%|████| 82/82 [00:08<00:00,  9.35it/s, train_loss=0.000378, valid_loss=0.000624]\n",
      "Epoch: 14/40...: 100%|████| 82/82 [00:08<00:00,  9.29it/s, train_loss=0.000313, valid_loss=0.000569]\n",
      "Epoch: 15/40...: 100%|████| 82/82 [00:08<00:00,  9.26it/s, train_loss=0.000292, valid_loss=0.000467]\n",
      "Epoch: 16/40...: 100%|████| 82/82 [00:08<00:00,  9.41it/s, train_loss=0.000256, valid_loss=0.000481]\n",
      "Epoch: 17/40...: 100%|████| 82/82 [00:08<00:00,  9.26it/s, train_loss=0.000277, valid_loss=0.000503]\n",
      "Epoch: 18/40...: 100%|████| 82/82 [00:08<00:00,  9.58it/s, train_loss=0.000307, valid_loss=0.000567]\n",
      "Epoch: 19/40...: 100%|█████| 82/82 [00:08<00:00,  9.26it/s, train_loss=0.000294, valid_loss=0.00049]\n",
      "Epoch: 20/40...: 100%|█████| 82/82 [00:08<00:00,  9.54it/s, train_loss=0.00023, valid_loss=0.000582]\n",
      "Epoch: 21/40...: 100%|████| 82/82 [00:08<00:00,  9.52it/s, train_loss=0.000235, valid_loss=0.000525]\n",
      "Epoch: 22/40...: 100%|████| 82/82 [00:08<00:00,  9.54it/s, train_loss=0.000396, valid_loss=0.000624]\n",
      "Epoch: 23/40...: 100%|████| 82/82 [00:08<00:00,  9.35it/s, train_loss=0.000323, valid_loss=0.000499]\n",
      "Epoch: 24/40...: 100%|████| 82/82 [00:08<00:00,  9.47it/s, train_loss=0.000205, valid_loss=0.000542]\n",
      "Epoch: 25/40...: 100%|████| 82/82 [00:08<00:00,  9.37it/s, train_loss=0.000247, valid_loss=0.000545]\n",
      "Epoch: 26/40...: 100%|████| 82/82 [00:08<00:00,  9.44it/s, train_loss=0.000226, valid_loss=0.000446]\n",
      "Epoch: 27/40...: 100%|████| 82/82 [00:08<00:00,  9.34it/s, train_loss=0.000185, valid_loss=0.000478]\n",
      "Epoch: 28/40...: 100%|████| 82/82 [00:08<00:00,  9.47it/s, train_loss=0.000181, valid_loss=0.000529]\n",
      "Epoch: 29/40...: 100%|████| 82/82 [00:08<00:00,  9.51it/s, train_loss=0.000171, valid_loss=0.000538]\n",
      "Epoch: 30/40...: 100%|████| 82/82 [00:08<00:00,  9.37it/s, train_loss=0.000201, valid_loss=0.000451]\n",
      "Epoch: 31/40...: 100%|████| 82/82 [00:08<00:00,  9.49it/s, train_loss=0.000249, valid_loss=0.000611]\n",
      "Epoch: 32/40...: 100%|████| 82/82 [00:08<00:00,  9.34it/s, train_loss=0.000182, valid_loss=0.000461]\n",
      "Epoch: 33/40...: 100%|█████| 82/82 [00:08<00:00,  9.35it/s, train_loss=0.00021, valid_loss=0.000473]\n",
      "Epoch: 34/40...: 100%|████| 82/82 [00:08<00:00,  9.49it/s, train_loss=0.000156, valid_loss=0.000514]\n",
      "Epoch: 35/40...: 100%|████| 82/82 [00:08<00:00,  9.37it/s, train_loss=0.000189, valid_loss=0.000507]\n",
      "Epoch: 36/40...: 100%|████| 82/82 [00:08<00:00,  9.52it/s, train_loss=0.000165, valid_loss=0.000536]\n",
      "Epoch: 37/40...: 100%|█████| 82/82 [00:08<00:00,  9.46it/s, train_loss=0.00016, valid_loss=0.000464]\n",
      "Epoch: 38/40...: 100%|████| 82/82 [00:08<00:00,  9.45it/s, train_loss=0.000142, valid_loss=0.000495]\n",
      "Epoch: 39/40...: 100%|████| 82/82 [00:08<00:00,  9.44it/s, train_loss=0.000154, valid_loss=0.000468]\n",
      "Epoch: 40/40...: 100%|████| 82/82 [00:08<00:00,  9.38it/s, train_loss=0.000175, valid_loss=0.000488]\n"
     ]
    }
   ],
   "source": [
    "random_seed_set(42)\n",
    "records = run_model_hpo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7d0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:52:06.844501Z",
     "start_time": "2021-12-22T08:52:06.830928Z"
    }
   },
   "source": [
    "## find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33e0fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T09:36:03.795921Z",
     "start_time": "2022-01-05T09:36:03.765758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>number_epoch</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>drop_prob</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size    lr  number_epoch  hidden_dim  n_layers  drop_prob  \\\n",
       "1         256  0.01            40          64         2          0   \n",
       "5         256  0.01            40          64         3          0   \n",
       "3         256  0.01            40         128         3          0   \n",
       "0         512  0.01            40         128         3          0   \n",
       "4         256  0.01            40         128         2          0   \n",
       "2         512  0.01            40          64         2          0   \n",
       "\n",
       "   weight_decay  valid_loss  \n",
       "1             0    0.000390  \n",
       "5             0    0.000417  \n",
       "3             0    0.000559  \n",
       "0             0    0.000695  \n",
       "4             0    0.001726  \n",
       "2             0    0.042352  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = pd.DataFrame(records).sort_values(by='valid_loss')\n",
    "records.to_csv('./records/DARNet_records.csv', mode='a', index=False, header=False)\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded47372",
   "metadata": {},
   "source": [
    "## retrain a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19f91de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T09:12:07.989906Z",
     "start_time": "2022-01-19T09:12:07.964330Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, valid_x, valid_y, input_size, seq_len,\n",
    "                target_len, mse_thresh, hidden_dim, n_layers, number_epoch,\n",
    "                batch_size, lr, drop_prob, weight_decay):\n",
    "    while (1):\n",
    "        model = DARNet(input_size, [64, 64, 64], seq_len, [64, 64, 64])\n",
    "        model = model.to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=lr,\n",
    "                                     weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98)\n",
    "        # scheduler = SchedulerCosineDecayWarmup(optimizer, lr, 10, number_epoch)\n",
    "        valid_loss_min = np.Inf\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(train_x),\n",
    "                                      torch.FloatTensor(train_y))\n",
    "        valid_dataset = TensorDataset(torch.FloatTensor(valid_x),\n",
    "                                      torch.FloatTensor(valid_y))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True)\n",
    "        train_losses = list()\n",
    "\n",
    "        num_without_imp = 0\n",
    "\n",
    "        train_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        #train\n",
    "        for epoch in range(1, number_epoch + 1):\n",
    "            loop = tqdm(enumerate(train_loader),\n",
    "                        total=len(train_loader),\n",
    "                        leave=True,\n",
    "                        ncols=100)\n",
    "            for i, (inputs, labels) in loop:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                encoder_inputs = inputs\n",
    "                decoder_inputs = torch.cat(\n",
    "                    (inputs[:, -1:, :], labels[:, :-1, :]), dim=1)\n",
    "                outputs = model(encoder_inputs, decoder_inputs)\n",
    "                loss = criterion(outputs, labels[:, :, -1])\n",
    "                train_losses.append(loss.item)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # eval\n",
    "                if i % 5 == 0:\n",
    "                    num_without_imp = num_without_imp + 1\n",
    "                    valid_losses = list()\n",
    "                    model.eval()\n",
    "                    for inp, lab in valid_loader:\n",
    "                        inp = inp.to(device)\n",
    "                        lab = lab.to(device)\n",
    "                        encoder_inp = inp\n",
    "                        decoder_inp = torch.cat(\n",
    "                            (inp[:, -1:, :], lab[:, :-1, :]), dim=1)\n",
    "                        out = model(encoder_inp, decoder_inp)\n",
    "                        valid_loss = criterion(out, lab[:, :, -1])\n",
    "                        valid_losses.append(valid_loss.item())\n",
    "                    model.train()\n",
    "                    loop.set_description(\"Epoch: {}/{}...\".format(\n",
    "                        epoch, number_epoch))\n",
    "                    loop.set_postfix(train_loss=loss.item(),\n",
    "                                     valid_loss=np.mean(valid_losses))\n",
    "                    train_loss_list.append(loss.item())\n",
    "                    valid_loss_list.append(np.mean(valid_losses))\n",
    "                    if np.mean(valid_losses) < valid_loss_min:\n",
    "                        num_without_imp = 0\n",
    "                        torch.save(model.state_dict(),\n",
    "                                   \"./model/DARNet_state_dict.pt\")\n",
    "                        valid_loss_min = np.mean(valid_losses)\n",
    "            scheduler.step()\n",
    "        if valid_loss_min < mse_thresh:\n",
    "            break\n",
    "    return model, train_loss_list, valid_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb8fc2",
   "metadata": {},
   "source": [
    "## test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "978a8b73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T09:12:09.157380Z",
     "start_time": "2022-01-19T09:12:09.139102Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_x, test_y, scaler_y, seq_len, target_len,\n",
    "               batch_size):\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(test_x),\n",
    "                                 torch.FloatTensor(test_y))\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=True)\n",
    "    model.load_state_dict(torch.load('./model/DARNet_state_dict.pt'))\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            encoder_inputs = inputs\n",
    "            decoder_inputs = torch.cat((inputs[:, -1:, :], labels[:, :-1, :]),\n",
    "                                       dim=1)\n",
    "            outputs = model(encoder_inputs, decoder_inputs)\n",
    "            y_pred += outputs.cpu().numpy().flatten().tolist()\n",
    "            y_true += labels[:, :, -1].cpu().numpy().flatten().tolist()\n",
    "    y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "    y_true = np.array(y_true).reshape(-1, 1)\n",
    "    #     pdb.set_trace()\n",
    "    load_pred = scaler_y.inverse_transform(y_pred)\n",
    "    load_true = scaler_y.inverse_transform(y_true)\n",
    "    mean_pred = np.mean(load_pred)\n",
    "    mean_true = np.mean(load_true)\n",
    "    MAPE = np.mean(np.abs(load_true - load_pred) / load_true)\n",
    "    SMAPE = 2 * np.mean(\n",
    "        np.abs(load_true - load_pred) / (load_true + load_pred))\n",
    "    MAE = np.mean(np.abs(load_true - load_pred))\n",
    "    RMSE = np.sqrt(np.mean(np.square(load_true - load_pred)))\n",
    "    RRSE = np.sqrt(np.sum(np.square(load_true - load_pred))) / np.sqrt(\n",
    "        np.sum(np.square(load_true - mean_true)))\n",
    "    CORR = np.mean(\n",
    "        np.sum((load_true - mean_true) * (load_pred - mean_pred)) /\n",
    "        np.sqrt(np.sum(\n",
    "            (load_true - mean_true)**2 * (load_pred - mean_pred)**2)))\n",
    "    return MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c46ae7",
   "metadata": {},
   "source": [
    "## RUN model retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1770307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T09:12:11.539620Z",
     "start_time": "2022-01-19T09:12:11.523206Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_retraining(seq_len=seq_len,\n",
    "                         target_len=target_len,\n",
    "                         mse_thresh=mse_thresh):\n",
    "    train_data = data[:int(0.8 * len(data))]\n",
    "    train_data, scaler, scaler_y = normalization(train_data)\n",
    "    train_x, train_y = series_to_supervise(train_data, seq_len, target_len)\n",
    "\n",
    "    valid_x = train_x[int(0.8 * len(train_x)):]\n",
    "    valid_y = train_y[int(0.8 * len(train_y)):]\n",
    "    train_x = train_x[:int(0.8 * len(train_x))]\n",
    "    train_y = train_y[:int(0.8 * len(train_y))]\n",
    "    input_size = train_x.shape[2]\n",
    "\n",
    "    #     hyper-parameters define\n",
    "    batch_size = 256\n",
    "    lr = 0.01\n",
    "    number_epoch = 80\n",
    "    hidden_dim = 64\n",
    "    n_layers = 2\n",
    "    drop_prob = 0.5\n",
    "    weight_decay = 0\n",
    "    mse_thresh = 0.01\n",
    "    \n",
    "    \n",
    "    model, train_loss_list, valid_loss_list = train_model(\n",
    "        train_x, train_y, valid_x, valid_y, input_size, seq_len, target_len,\n",
    "        mse_thresh, hidden_dim, n_layers, number_epoch, batch_size, lr,\n",
    "        drop_prob, weight_decay)\n",
    "\n",
    "    # plot training process\n",
    "    plt.plot(train_loss_list[10:], 'm', label='train_loss')\n",
    "    plt.plot(valid_loss_list[10:], 'g', label='valid_loss')\n",
    "    plt.grid('both')\n",
    "    plt.legend()\n",
    "    \n",
    "    # test\n",
    "\n",
    "    test_data = data[int(0.8 * len(data)):]\n",
    "    test_data = scaler.transform(test_data)\n",
    "    test_x, test_y = series_to_supervise(test_data, seq_len, target_len)\n",
    "    MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true = test_model(\n",
    "        model, test_x, test_y, scaler_y, seq_len, target_len, batch_size)\n",
    "    return MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3e349",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-19T09:12:28.042Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised data: shape of x: (25232, 72, 16), shape of y: (25232, 24, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/80...:  45%|████     | 35/78 [00:11<00:09,  4.44it/s, train_loss=0.0213, valid_loss=0.0398]"
     ]
    }
   ],
   "source": [
    "random_seed_set(16)\n",
    "MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true = run_model_retraining()\n",
    "print('MAPE:{:.6f},SMAPE:{:.6f},MAE:{:.6f},RMSE:{:.6f},RRSE:{:.6f},CORR:{:.6f}'.format(MAPE, SMAPE, MAE, RMSE, RRSE, CORR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd05c1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-19T09:12:28.044Z"
    }
   },
   "outputs": [],
   "source": [
    "print('MAPE:{:.6f},SMAPE:{:.6f},MAE:{:.6f},RMSE:{:.6f},RRSE:{:.6f},CORR:{:.6f}'.format(MAPE, SMAPE, MAE, RMSE, RRSE, CORR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7590f",
   "metadata": {},
   "source": [
    "## figure plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51226973",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-19T09:12:28.048Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "load_pred = load_pred.reshape(-1, 24)\n",
    "load_true = load_true.reshape(-1, 24)\n",
    "plt.plot(load_pred[:240, 23], 'm')\n",
    "plt.plot(load_true[:240, 23], 'g')\n",
    "plt.ylim(ymin=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
