{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0909ba",
   "metadata": {},
   "source": [
    "# LSTM for forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b7f90",
   "metadata": {},
   "source": [
    "1. 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3987cf65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.131435Z",
     "start_time": "2022-01-09T08:15:12.119015Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88e5ba",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bddde09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.182661Z",
     "start_time": "2022-01-09T08:15:13.133186Z"
    }
   },
   "outputs": [],
   "source": [
    "url = '../data/beijing.csv'\n",
    "data = pd.read_csv(url, sep=',', index_col='time')\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aa1e7",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed7f131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.187588Z",
     "start_time": "2022-01-09T08:15:13.184351Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    \"\"\"\n",
    "    data: original data with load\n",
    "    return: normalized data, scaler of load\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit_transform(data[[data.columns[-1]]])\n",
    "    return normalized_data, scaler, scaler_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6c94b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T07:45:44.192666Z",
     "start_time": "2021-12-22T07:45:44.188985Z"
    }
   },
   "source": [
    "## build supervised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0abfb3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.193138Z",
     "start_time": "2022-01-09T08:15:13.189281Z"
    }
   },
   "outputs": [],
   "source": [
    "def series_to_supervise(data, seq_len, target_len):\n",
    "    \"\"\"\n",
    "    convert series data to supervised data\n",
    "    :param data: original data\n",
    "    :param seq_len: length of input sequence\n",
    "    :param target_len: length of ouput sequence\n",
    "    :return: return two ndarrays-- input and output in format suitable to feed to LSTM\n",
    "    \"\"\"\n",
    "    dim_0 = data.shape[0] - seq_len - target_len + 1\n",
    "    dim_1 = data.shape[1]\n",
    "    x = np.zeros((dim_0, seq_len, dim_1))\n",
    "    y = np.zeros((dim_0, target_len))\n",
    "    for i in range(dim_0):\n",
    "        x[i] = data[i:i + seq_len]\n",
    "        y[i] = data[i + seq_len:i + seq_len + target_len, -1]\n",
    "    print(\"supervised data: shape of x: {}, shape of y: {}\".format(x.shape, y.shape))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb1a52",
   "metadata": {},
   "source": [
    "## 5-folds TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47b1b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.197936Z",
     "start_time": "2022-01-09T08:15:13.194386Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_series_split(X, Y, n_split=5):\n",
    "    \"\"\"\n",
    "    X: features, size * seq_len * feature_num\n",
    "    Y: labels, size * target_len\n",
    "    return: list of train_x, test_x, train_y, test_y\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    train_x_list = list()\n",
    "    valid_x_list = list()\n",
    "    train_y_list = list()\n",
    "    valid_y_list = list()\n",
    "    for train_index, valid_index in tscv.split(X):\n",
    "        train_x_list.append(X[train_index])\n",
    "        train_y_list.append(Y[train_index])\n",
    "        valid_x_list.append(X[valid_index])\n",
    "        valid_y_list.append(Y[valid_index])\n",
    "    return train_x_list, train_y_list, valid_x_list, valid_y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25128d",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8116d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.204723Z",
     "start_time": "2022-01-09T08:15:13.199423Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,\n",
    "                 drop_prob):\n",
    "        super(LSTM, self).__init__()\n",
    "        # model parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        # layes\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=drop_prob)\n",
    "        self.fc = nn.Sequential()\n",
    "        \n",
    "        input_size = hidden_dim\n",
    "        i = 0\n",
    "        while (input_size > 8):\n",
    "            self.fc.add_module('linear{}'.format(i), nn.Linear(input_size, round(input_size / 2)))\n",
    "            self.fc.add_module('relu{}'.format(i), nn.ReLU())\n",
    "            input_size = round(input_size / 2)\n",
    "            i += 1\n",
    "        self.fc.add_module('linear{}'.format(i), nn.Linear(input_size, 1))\n",
    "\n",
    "    # predict using outputs of the last 24 steps\n",
    "    def forward(self, x):\n",
    "\n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -self.output_size:, :]\n",
    "\n",
    "        out = self.fc(lstm_out)\n",
    "\n",
    "        out = out.reshape(x.shape[0], -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c4c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:30:19.661769Z",
     "start_time": "2021-12-22T08:30:19.656770Z"
    }
   },
   "source": [
    "## model training for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3b1b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.215958Z",
     "start_time": "2022-01-09T08:15:13.206159Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_hpo(train_x_list, train_y_list, valid_x_list, valid_y_list,\n",
    "                    input_size, output_size, mse_thresh, batch_size, lr,\n",
    "                    number_epoch, hidden_dim, n_layers, drop_prob, weight_decay):\n",
    "    valid_loss_list = []\n",
    "    for num in range(len(train_x_list)):\n",
    "        while (1):\n",
    "            model = LSTM(input_size, output_size, hidden_dim, n_layers,\n",
    "                           drop_prob)\n",
    "            model.to(device=device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98)\n",
    "            valid_loss_min = np.Inf\n",
    "            print('cross-validation dataset {}'.format(num))\n",
    "            train_x = train_x_list[num]\n",
    "            train_y = train_y_list[num]\n",
    "            valid_x = valid_x_list[num]\n",
    "            valid_y = valid_y_list[num]\n",
    "            train_dataset = TensorDataset(torch.FloatTensor(train_x),\n",
    "                                          torch.FloatTensor(train_y))\n",
    "            valid_dataset = TensorDataset(torch.FloatTensor(valid_x),\n",
    "                                          torch.FloatTensor(valid_y))\n",
    "            train_loader = DataLoader(dataset=train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "            valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "            num_without_imp = 0\n",
    "            # training process\n",
    "            for epoch in range(1, number_epoch + 1):\n",
    "                loop = tqdm(enumerate(train_loader),\n",
    "                            total=len(train_loader),\n",
    "                            leave=True)\n",
    "                for i, (inputs, labels) in loop:\n",
    "                    inputs = inputs.to(device=device)\n",
    "                    labels = labels.to(device=device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if i % 5 == 0:\n",
    "                        num_without_imp = num_without_imp + 1\n",
    "                        valid_losses = list()\n",
    "                        model.eval()\n",
    "                        for inp, lab in valid_loader:\n",
    "                            inp = inp.to(device)\n",
    "                            lab = lab.to(device)\n",
    "                            out = model(inp)\n",
    "                            valid_loss = criterion(out, lab)\n",
    "                            valid_losses.append(valid_loss.item())\n",
    "\n",
    "                        model.train()\n",
    "                        loop.set_description(\"Epoch: {}/{}\".format(\n",
    "                            epoch, number_epoch))\n",
    "                        loop.set_postfix(train_loss=loss.item(),\n",
    "                                         valid_loss=np.mean(valid_losses))\n",
    "                        if np.mean(valid_losses) < valid_loss_min:\n",
    "                            num_without_imp = 0\n",
    "                            valid_loss_min = np.mean(valid_losses)\n",
    "                scheduler.step()\n",
    "                if num_without_imp >= 50:\n",
    "#                     break\n",
    "                    pass\n",
    "            if valid_loss_min < mse_thresh:\n",
    "                valid_loss_list.append(valid_loss_min)\n",
    "                break\n",
    "    return np.mean(valid_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a50b9",
   "metadata": {},
   "source": [
    "## hyper-parameters config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d7c6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.221327Z",
     "start_time": "2022-01-09T08:15:13.216973Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = 72\n",
    "target_len = 24\n",
    "mse_thresh = 0.05\n",
    "\n",
    "\n",
    "def model_config():\n",
    "    batch_sizes = [256]\n",
    "    lrs = [0.01]\n",
    "    number_epochs = [30]\n",
    "    hidden_dims = [64, 128, 256]\n",
    "    n_layers = [2, 3]\n",
    "    drop_prob = [0]\n",
    "    weight_decays = [0]\n",
    "    configs = list()\n",
    "    for i in batch_sizes:\n",
    "        for j in lrs:\n",
    "            for k in number_epochs:\n",
    "                for l in hidden_dims:\n",
    "                    for m in n_layers:\n",
    "                        for n in drop_prob:\n",
    "                            for o in weight_decays:\n",
    "                                configs.append({\n",
    "                                    'batch_size': i,\n",
    "                                    'lr': j,\n",
    "                                    'number_epoch': k,\n",
    "                                    'hidden_dim': l,\n",
    "                                    'n_layers': m,\n",
    "                                    'drop_prob': n,\n",
    "                                    'weight_decay': o,\n",
    "                                })\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883d48b",
   "metadata": {},
   "source": [
    "## random search for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52da5ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:13.228371Z",
     "start_time": "2022-01-09T08:15:13.222440Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_hpo(seq_len=seq_len,\n",
    "                  target_len=target_len,\n",
    "                  mse_thresh=mse_thresh):\n",
    "    train_data = data[:int(0.8 * len(data))]\n",
    "    train_data, _, _ = normalization(train_data)\n",
    "    train_x, train_y = series_to_supervise(train_data, seq_len, target_len)\n",
    "    train_x_list, train_y_list, valid_x_list, valid_y_list = time_series_split(\n",
    "        train_x, train_y)\n",
    "    #     with enough data\n",
    "    train_x_list = train_x_list[-1:]\n",
    "    train_y_list = train_y_list[-1:]\n",
    "    valid_x_list = valid_x_list[-1:]\n",
    "    valid_y_list = valid_y_list[-1:]\n",
    "\n",
    "    configs = model_config()\n",
    "    records = []\n",
    "    input_size = train_x.shape[2]\n",
    "    output_size = target_len\n",
    "    for i in range(6):\n",
    "        config = random.choice(configs)\n",
    "        configs.remove(config)\n",
    "        batch_size = config['batch_size']\n",
    "        lr = config['lr']\n",
    "        number_epoch = config['number_epoch']\n",
    "        hidden_dim = config['hidden_dim']\n",
    "        n_layers = config['n_layers']\n",
    "        drop_prob = config['drop_prob']\n",
    "        weight_decay = config['weight_decay']\n",
    "        print(\n",
    "            \"model config: batch_size-{}, lr-{}, number_epoch-{}, hidden_dim-{}, n_layers-{},drop_prob-{},weight_decay-{}\"\n",
    "            .format(batch_size, lr, number_epoch, hidden_dim, n_layers,\n",
    "                    drop_prob, weight_decay))\n",
    "        valid_loss = train_model_hpo(train_x_list, train_y_list, valid_x_list,\n",
    "                                     valid_y_list, input_size, output_size,\n",
    "                                     mse_thresh, batch_size, lr, number_epoch,\n",
    "                                     hidden_dim, n_layers, drop_prob, weight_decay)\n",
    "        records.append({\n",
    "            'batch_size': batch_size,\n",
    "            'lr': lr,\n",
    "            'number_epoch': number_epoch,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'n_layers': n_layers,\n",
    "            'drop_prob': drop_prob,\n",
    "            'weight_decay': weight_decay,\n",
    "            'valid_loss': valid_loss\n",
    "        })\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ba1b9",
   "metadata": {},
   "source": [
    "## run random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8503f596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T12:30:40.887308Z",
     "start_time": "2021-12-29T12:21:41.768493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised data: shape of x: (25232, 72, 16), shape of y: (25232, 24)\n",
      "model config: batch_size-256, lr-0.01, number_epoch-30, hidden_dim-64, n_layers-2,drop_prob-0,weight_decay-0\n",
      "cross-validation dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|██████████| 83/83 [00:03<00:00, 22.58it/s, train_loss=0.00635, valid_loss=0.00776]\n",
      "Epoch: 2/30: 100%|██████████| 83/83 [00:02<00:00, 32.64it/s, train_loss=0.00163, valid_loss=0.00317]\n",
      "Epoch: 3/30: 100%|██████████| 83/83 [00:02<00:00, 35.80it/s, train_loss=0.00163, valid_loss=0.0035] \n",
      "Epoch: 4/30: 100%|██████████| 83/83 [00:01<00:00, 51.87it/s, train_loss=0.00212, valid_loss=0.00273]\n",
      "Epoch: 5/30: 100%|██████████| 83/83 [00:01<00:00, 52.00it/s, train_loss=0.00154, valid_loss=0.00284]\n",
      "Epoch: 6/30: 100%|██████████| 83/83 [00:01<00:00, 49.68it/s, train_loss=0.00153, valid_loss=0.00272]\n",
      "Epoch: 7/30: 100%|██████████| 83/83 [00:01<00:00, 51.54it/s, train_loss=0.00148, valid_loss=0.00264]\n",
      "Epoch: 8/30: 100%|██████████| 83/83 [00:01<00:00, 49.53it/s, train_loss=0.00123, valid_loss=0.0027] \n",
      "Epoch: 9/30: 100%|██████████| 83/83 [00:01<00:00, 51.76it/s, train_loss=0.00122, valid_loss=0.00254] \n",
      "Epoch: 10/30: 100%|██████████| 83/83 [00:01<00:00, 51.80it/s, train_loss=0.00154, valid_loss=0.00283]\n",
      "Epoch: 11/30: 100%|██████████| 83/83 [00:01<00:00, 49.76it/s, train_loss=0.00185, valid_loss=0.00256]\n",
      "Epoch: 12/30: 100%|██████████| 83/83 [00:01<00:00, 51.89it/s, train_loss=0.00106, valid_loss=0.00256]\n",
      "Epoch: 13/30: 100%|██████████| 83/83 [00:01<00:00, 51.87it/s, train_loss=0.00114, valid_loss=0.00283] \n",
      "Epoch: 14/30: 100%|██████████| 83/83 [00:01<00:00, 49.35it/s, train_loss=0.000897, valid_loss=0.00262]\n",
      "Epoch: 15/30: 100%|██████████| 83/83 [00:01<00:00, 51.98it/s, train_loss=0.00136, valid_loss=0.00262]\n",
      "Epoch: 16/30: 100%|██████████| 83/83 [00:01<00:00, 49.86it/s, train_loss=0.00132, valid_loss=0.0027] \n",
      "Epoch: 17/30: 100%|██████████| 83/83 [00:01<00:00, 51.84it/s, train_loss=0.000985, valid_loss=0.00262]\n",
      "Epoch: 18/30: 100%|██████████| 83/83 [00:01<00:00, 52.07it/s, train_loss=0.00131, valid_loss=0.00269] \n",
      "Epoch: 19/30: 100%|██████████| 83/83 [00:01<00:00, 49.93it/s, train_loss=0.000943, valid_loss=0.00275]\n",
      "Epoch: 20/30: 100%|██████████| 83/83 [00:01<00:00, 51.97it/s, train_loss=0.000884, valid_loss=0.00292]\n",
      "Epoch: 21/30: 100%|██████████| 83/83 [00:01<00:00, 49.97it/s, train_loss=0.000757, valid_loss=0.0031] \n",
      "Epoch: 22/30: 100%|██████████| 83/83 [00:01<00:00, 51.88it/s, train_loss=0.000791, valid_loss=0.00335]\n",
      "Epoch: 23/30: 100%|██████████| 83/83 [00:01<00:00, 52.10it/s, train_loss=0.000537, valid_loss=0.00341]\n",
      "Epoch: 24/30: 100%|██████████| 83/83 [00:01<00:00, 49.81it/s, train_loss=0.000535, valid_loss=0.00355]\n",
      "Epoch: 25/30: 100%|██████████| 83/83 [00:01<00:00, 52.00it/s, train_loss=0.000475, valid_loss=0.00326]\n",
      "Epoch: 26/30: 100%|██████████| 83/83 [00:01<00:00, 51.85it/s, train_loss=0.000379, valid_loss=0.00356]\n",
      "Epoch: 27/30: 100%|██████████| 83/83 [00:01<00:00, 49.60it/s, train_loss=0.000376, valid_loss=0.00344]\n",
      "Epoch: 28/30: 100%|██████████| 83/83 [00:01<00:00, 51.68it/s, train_loss=0.000337, valid_loss=0.00354]\n",
      "Epoch: 29/30: 100%|██████████| 83/83 [00:01<00:00, 49.60it/s, train_loss=0.000328, valid_loss=0.00348]\n",
      "Epoch: 30/30: 100%|██████████| 83/83 [00:01<00:00, 51.96it/s, train_loss=0.000328, valid_loss=0.00378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-30, hidden_dim-128, n_layers-2,drop_prob-0,weight_decay-0\n",
      "cross-validation dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|██████████| 83/83 [00:02<00:00, 33.46it/s, train_loss=0.00394, valid_loss=0.00464]\n",
      "Epoch: 2/30: 100%|██████████| 83/83 [00:02<00:00, 32.62it/s, train_loss=0.00166, valid_loss=0.00333]\n",
      "Epoch: 3/30: 100%|██████████| 83/83 [00:02<00:00, 33.56it/s, train_loss=0.00164, valid_loss=0.00351]\n",
      "Epoch: 4/30: 100%|██████████| 83/83 [00:02<00:00, 32.53it/s, train_loss=0.00138, valid_loss=0.0029] \n",
      "Epoch: 5/30: 100%|██████████| 83/83 [00:02<00:00, 33.60it/s, train_loss=0.00188, valid_loss=0.00254]\n",
      "Epoch: 6/30: 100%|██████████| 83/83 [00:02<00:00, 32.50it/s, train_loss=0.00131, valid_loss=0.00271]\n",
      "Epoch: 7/30: 100%|██████████| 83/83 [00:02<00:00, 31.32it/s, train_loss=0.00145, valid_loss=0.00257]\n",
      "Epoch: 8/30: 100%|██████████| 83/83 [00:02<00:00, 32.26it/s, train_loss=0.00113, valid_loss=0.00262] \n",
      "Epoch: 9/30: 100%|██████████| 83/83 [00:02<00:00, 31.51it/s, train_loss=0.000925, valid_loss=0.00309]\n",
      "Epoch: 10/30: 100%|██████████| 83/83 [00:02<00:00, 32.34it/s, train_loss=0.000905, valid_loss=0.00311]\n",
      "Epoch: 11/30: 100%|██████████| 83/83 [00:02<00:00, 32.11it/s, train_loss=0.000969, valid_loss=0.00286]\n",
      "Epoch: 12/30: 100%|██████████| 83/83 [00:02<00:00, 31.79it/s, train_loss=0.0006, valid_loss=0.00289]  \n",
      "Epoch: 13/30: 100%|██████████| 83/83 [00:02<00:00, 32.28it/s, train_loss=0.000593, valid_loss=0.00293]\n",
      "Epoch: 14/30: 100%|██████████| 83/83 [00:02<00:00, 32.23it/s, train_loss=0.000459, valid_loss=0.00315]\n",
      "Epoch: 15/30: 100%|██████████| 83/83 [00:02<00:00, 31.52it/s, train_loss=0.000269, valid_loss=0.00325]\n",
      "Epoch: 16/30: 100%|██████████| 83/83 [00:02<00:00, 32.00it/s, train_loss=0.000267, valid_loss=0.00327]\n",
      "Epoch: 17/30: 100%|██████████| 83/83 [00:02<00:00, 31.30it/s, train_loss=0.00034, valid_loss=0.00312] \n",
      "Epoch: 18/30: 100%|██████████| 83/83 [00:02<00:00, 32.12it/s, train_loss=0.000172, valid_loss=0.00328]\n",
      "Epoch: 19/30: 100%|██████████| 83/83 [00:02<00:00, 32.29it/s, train_loss=0.000143, valid_loss=0.00345]\n",
      "Epoch: 20/30: 100%|██████████| 83/83 [00:02<00:00, 31.38it/s, train_loss=0.000133, valid_loss=0.0035] \n",
      "Epoch: 21/30: 100%|██████████| 83/83 [00:02<00:00, 32.37it/s, train_loss=0.000109, valid_loss=0.00361]\n",
      "Epoch: 22/30: 100%|██████████| 83/83 [00:02<00:00, 31.58it/s, train_loss=8.26e-5, valid_loss=0.00358] \n",
      "Epoch: 23/30: 100%|██████████| 83/83 [00:02<00:00, 32.61it/s, train_loss=7.45e-5, valid_loss=0.00356] \n",
      "Epoch: 24/30: 100%|██████████| 83/83 [00:02<00:00, 32.28it/s, train_loss=6.86e-5, valid_loss=0.00348]\n",
      "Epoch: 25/30: 100%|██████████| 83/83 [00:02<00:00, 31.67it/s, train_loss=5.7e-5, valid_loss=0.00359] \n",
      "Epoch: 26/30: 100%|██████████| 83/83 [00:02<00:00, 32.69it/s, train_loss=5.43e-5, valid_loss=0.0036] \n",
      "Epoch: 27/30: 100%|██████████| 83/83 [00:02<00:00, 31.60it/s, train_loss=5.11e-5, valid_loss=0.00366]\n",
      "Epoch: 28/30: 100%|██████████| 83/83 [00:02<00:00, 32.44it/s, train_loss=4.8e-5, valid_loss=0.00358] \n",
      "Epoch: 29/30: 100%|██████████| 83/83 [00:02<00:00, 32.06it/s, train_loss=4.63e-5, valid_loss=0.00362]\n",
      "Epoch: 30/30: 100%|██████████| 83/83 [00:02<00:00, 31.52it/s, train_loss=5.55e-5, valid_loss=0.00364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-30, hidden_dim-256, n_layers-3,drop_prob-0,weight_decay-0\n",
      "cross-validation dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|██████████| 83/83 [00:03<00:00, 23.22it/s, train_loss=0.0328, valid_loss=0.035] \n",
      "Epoch: 2/30: 100%|██████████| 83/83 [00:03<00:00, 23.19it/s, train_loss=0.0309, valid_loss=0.0353]\n",
      "Epoch: 3/30: 100%|██████████| 83/83 [00:03<00:00, 22.87it/s, train_loss=0.0298, valid_loss=0.0357]\n",
      "Epoch: 4/30: 100%|██████████| 83/83 [00:03<00:00, 23.17it/s, train_loss=0.0319, valid_loss=0.0351]\n",
      "Epoch: 5/30: 100%|██████████| 83/83 [00:03<00:00, 22.73it/s, train_loss=0.032, valid_loss=0.0355] \n",
      "Epoch: 6/30: 100%|██████████| 83/83 [00:03<00:00, 23.72it/s, train_loss=0.0299, valid_loss=0.0357]\n",
      "Epoch: 7/30: 100%|██████████| 83/83 [00:03<00:00, 23.36it/s, train_loss=0.0304, valid_loss=0.0351]\n",
      "Epoch: 8/30: 100%|██████████| 83/83 [00:03<00:00, 22.48it/s, train_loss=0.0309, valid_loss=0.036] \n",
      "Epoch: 9/30: 100%|██████████| 83/83 [00:03<00:00, 23.33it/s, train_loss=0.0287, valid_loss=0.0351]\n",
      "Epoch: 10/30: 100%|██████████| 83/83 [00:03<00:00, 21.00it/s, train_loss=0.0325, valid_loss=0.0351]\n",
      "Epoch: 11/30: 100%|██████████| 83/83 [00:04<00:00, 19.23it/s, train_loss=0.0314, valid_loss=0.0351]\n",
      "Epoch: 12/30: 100%|██████████| 83/83 [00:04<00:00, 19.18it/s, train_loss=0.0329, valid_loss=0.0353]\n",
      "Epoch: 13/30: 100%|██████████| 83/83 [00:04<00:00, 19.14it/s, train_loss=0.0297, valid_loss=0.0349]\n",
      "Epoch: 14/30: 100%|██████████| 83/83 [00:04<00:00, 19.37it/s, train_loss=0.0329, valid_loss=0.0349]\n",
      "Epoch: 15/30: 100%|██████████| 83/83 [00:04<00:00, 19.28it/s, train_loss=0.0306, valid_loss=0.0357]\n",
      "Epoch: 16/30: 100%|██████████| 83/83 [00:04<00:00, 18.93it/s, train_loss=0.0292, valid_loss=0.0349]\n",
      "Epoch: 17/30: 100%|██████████| 83/83 [00:04<00:00, 19.38it/s, train_loss=0.0316, valid_loss=0.0348]\n",
      "Epoch: 18/30: 100%|██████████| 83/83 [00:04<00:00, 19.13it/s, train_loss=0.0302, valid_loss=0.0349]\n",
      "Epoch: 19/30: 100%|██████████| 83/83 [00:04<00:00, 19.52it/s, train_loss=0.033, valid_loss=0.035]  \n",
      "Epoch: 20/30: 100%|██████████| 83/83 [00:04<00:00, 19.39it/s, train_loss=0.0295, valid_loss=0.0352]\n",
      "Epoch: 21/30: 100%|██████████| 83/83 [00:04<00:00, 19.04it/s, train_loss=0.0303, valid_loss=0.0348]\n",
      "Epoch: 22/30: 100%|██████████| 83/83 [00:04<00:00, 19.10it/s, train_loss=0.0304, valid_loss=0.0355]\n",
      "Epoch: 23/30: 100%|██████████| 83/83 [00:04<00:00, 19.30it/s, train_loss=0.0299, valid_loss=0.0348]\n",
      "Epoch: 24/30: 100%|██████████| 83/83 [00:04<00:00, 19.60it/s, train_loss=0.0282, valid_loss=0.0357]\n",
      "Epoch: 25/30: 100%|██████████| 83/83 [00:04<00:00, 19.40it/s, train_loss=0.0305, valid_loss=0.035] \n",
      "Epoch: 26/30: 100%|██████████| 83/83 [00:04<00:00, 19.09it/s, train_loss=0.0311, valid_loss=0.0356]\n",
      "Epoch: 27/30: 100%|██████████| 83/83 [00:04<00:00, 19.45it/s, train_loss=0.0288, valid_loss=0.0353]\n",
      "Epoch: 28/30: 100%|██████████| 83/83 [00:04<00:00, 19.21it/s, train_loss=0.0307, valid_loss=0.0352]\n",
      "Epoch: 29/30: 100%|██████████| 83/83 [00:04<00:00, 19.12it/s, train_loss=0.03, valid_loss=0.0352]  \n",
      "Epoch: 30/30: 100%|██████████| 83/83 [00:04<00:00, 19.48it/s, train_loss=0.031, valid_loss=0.035]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-30, hidden_dim-128, n_layers-3,drop_prob-0,weight_decay-0\n",
      "cross-validation dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|██████████| 83/83 [00:03<00:00, 22.51it/s, train_loss=0.0331, valid_loss=0.0354]\n",
      "Epoch: 2/30: 100%|██████████| 83/83 [00:03<00:00, 23.11it/s, train_loss=0.0304, valid_loss=0.0353]\n",
      "Epoch: 3/30: 100%|██████████| 83/83 [00:03<00:00, 23.02it/s, train_loss=0.0309, valid_loss=0.0355]\n",
      "Epoch: 4/30: 100%|██████████| 83/83 [00:03<00:00, 22.68it/s, train_loss=0.0334, valid_loss=0.0355]\n",
      "Epoch: 5/30: 100%|██████████| 83/83 [00:03<00:00, 23.29it/s, train_loss=0.0315, valid_loss=0.0348]\n",
      "Epoch: 6/30: 100%|██████████| 83/83 [00:03<00:00, 22.46it/s, train_loss=0.031, valid_loss=0.0352] \n",
      "Epoch: 7/30: 100%|██████████| 83/83 [00:03<00:00, 22.76it/s, train_loss=0.0313, valid_loss=0.0359]\n",
      "Epoch: 8/30: 100%|██████████| 83/83 [00:03<00:00, 23.12it/s, train_loss=0.0309, valid_loss=0.0358]\n",
      "Epoch: 9/30: 100%|██████████| 83/83 [00:03<00:00, 22.64it/s, train_loss=0.03, valid_loss=0.0346]  \n",
      "Epoch: 10/30: 100%|██████████| 83/83 [00:03<00:00, 23.26it/s, train_loss=0.0317, valid_loss=0.0351]\n",
      "Epoch: 11/30: 100%|██████████| 83/83 [00:03<00:00, 22.79it/s, train_loss=0.0293, valid_loss=0.035] \n",
      "Epoch: 12/30: 100%|██████████| 83/83 [00:03<00:00, 23.24it/s, train_loss=0.0299, valid_loss=0.0351]\n",
      "Epoch: 13/30: 100%|██████████| 83/83 [00:03<00:00, 23.42it/s, train_loss=0.0297, valid_loss=0.0351]\n",
      "Epoch: 14/30: 100%|██████████| 83/83 [00:03<00:00, 22.60it/s, train_loss=0.0328, valid_loss=0.035] \n",
      "Epoch: 15/30: 100%|██████████| 83/83 [00:03<00:00, 23.16it/s, train_loss=0.0315, valid_loss=0.0356]\n",
      "Epoch: 16/30: 100%|██████████| 83/83 [00:03<00:00, 23.06it/s, train_loss=0.0329, valid_loss=0.0357]\n",
      "Epoch: 17/30: 100%|██████████| 83/83 [00:03<00:00, 22.91it/s, train_loss=0.0297, valid_loss=0.035] \n",
      "Epoch: 18/30: 100%|██████████| 83/83 [00:03<00:00, 22.93it/s, train_loss=0.0337, valid_loss=0.0354]\n",
      "Epoch: 19/30: 100%|██████████| 83/83 [00:03<00:00, 22.70it/s, train_loss=0.029, valid_loss=0.0351] \n",
      "Epoch: 20/30: 100%|██████████| 83/83 [00:03<00:00, 22.74it/s, train_loss=0.0297, valid_loss=0.0351]\n",
      "Epoch: 21/30: 100%|██████████| 83/83 [00:03<00:00, 23.26it/s, train_loss=0.0326, valid_loss=0.0347]\n",
      "Epoch: 22/30: 100%|██████████| 83/83 [00:03<00:00, 22.78it/s, train_loss=0.0298, valid_loss=0.0352]\n",
      "Epoch: 23/30: 100%|██████████| 83/83 [00:03<00:00, 23.57it/s, train_loss=0.0317, valid_loss=0.0353]\n",
      "Epoch: 24/30: 100%|██████████| 83/83 [00:03<00:00, 22.67it/s, train_loss=0.034, valid_loss=0.0351] \n",
      "Epoch: 25/30: 100%|██████████| 83/83 [00:03<00:00, 23.64it/s, train_loss=0.034, valid_loss=0.0354] \n",
      "Epoch: 26/30: 100%|██████████| 83/83 [00:03<00:00, 25.71it/s, train_loss=0.0327, valid_loss=0.0354]\n",
      "Epoch: 27/30: 100%|██████████| 83/83 [00:03<00:00, 25.56it/s, train_loss=0.0307, valid_loss=0.0348]\n",
      "Epoch: 28/30: 100%|██████████| 83/83 [00:03<00:00, 26.05it/s, train_loss=0.0299, valid_loss=0.0354]\n",
      "Epoch: 29/30: 100%|██████████| 83/83 [00:03<00:00, 26.02it/s, train_loss=0.0288, valid_loss=0.0355]\n",
      "Epoch: 30/30: 100%|██████████| 83/83 [00:03<00:00, 25.40it/s, train_loss=0.0307, valid_loss=0.0356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-30, hidden_dim-64, n_layers-3,drop_prob-0,weight_decay-0\n",
      "cross-validation dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|██████████| 83/83 [00:03<00:00, 25.70it/s, train_loss=0.0317, valid_loss=0.0348]\n",
      "Epoch: 2/30: 100%|██████████| 83/83 [00:03<00:00, 25.26it/s, train_loss=0.0323, valid_loss=0.0353]\n",
      "Epoch: 3/30: 100%|██████████| 83/83 [00:03<00:00, 25.53it/s, train_loss=0.0317, valid_loss=0.0353]\n",
      "Epoch: 4/30: 100%|██████████| 83/83 [00:03<00:00, 25.50it/s, train_loss=0.032, valid_loss=0.0352] \n",
      "Epoch: 5/30: 100%|██████████| 83/83 [00:03<00:00, 25.36it/s, train_loss=0.0317, valid_loss=0.0358]\n",
      "Epoch: 6/30: 100%|██████████| 83/83 [00:03<00:00, 25.70it/s, train_loss=0.00658, valid_loss=0.00795]\n",
      "Epoch: 7/30: 100%|██████████| 83/83 [00:03<00:00, 25.18it/s, train_loss=0.00283, valid_loss=0.00332]\n",
      "Epoch: 8/30: 100%|██████████| 83/83 [00:03<00:00, 25.67it/s, train_loss=0.00202, valid_loss=0.00333]\n",
      "Epoch: 9/30: 100%|██████████| 83/83 [00:03<00:00, 25.59it/s, train_loss=0.00182, valid_loss=0.00302]\n",
      "Epoch: 10/30: 100%|██████████| 83/83 [00:03<00:00, 24.72it/s, train_loss=0.0015, valid_loss=0.00266] \n",
      "Epoch: 11/30: 100%|██████████| 83/83 [00:03<00:00, 25.61it/s, train_loss=0.00189, valid_loss=0.00257]\n",
      "Epoch: 12/30: 100%|██████████| 83/83 [00:03<00:00, 26.20it/s, train_loss=0.00156, valid_loss=0.0027] \n",
      "Epoch: 13/30: 100%|██████████| 83/83 [00:03<00:00, 25.07it/s, train_loss=0.00134, valid_loss=0.00261]\n",
      "Epoch: 14/30: 100%|██████████| 83/83 [00:03<00:00, 25.74it/s, train_loss=0.00202, valid_loss=0.00255]\n",
      "Epoch: 15/30: 100%|██████████| 83/83 [00:03<00:00, 25.22it/s, train_loss=0.0014, valid_loss=0.00261] \n",
      "Epoch: 16/30: 100%|██████████| 83/83 [00:03<00:00, 25.59it/s, train_loss=0.00123, valid_loss=0.00253]\n",
      "Epoch: 17/30: 100%|██████████| 83/83 [00:03<00:00, 26.01it/s, train_loss=0.0013, valid_loss=0.0025]  \n",
      "Epoch: 18/30: 100%|██████████| 83/83 [00:03<00:00, 25.01it/s, train_loss=0.00177, valid_loss=0.00254]\n",
      "Epoch: 19/30: 100%|██████████| 83/83 [00:03<00:00, 25.64it/s, train_loss=0.0013, valid_loss=0.00257] \n",
      "Epoch: 20/30: 100%|██████████| 83/83 [00:03<00:00, 25.39it/s, train_loss=0.00157, valid_loss=0.00254]\n",
      "Epoch: 21/30: 100%|██████████| 83/83 [00:03<00:00, 25.77it/s, train_loss=0.00142, valid_loss=0.00282]\n",
      "Epoch: 22/30: 100%|██████████| 83/83 [00:03<00:00, 25.50it/s, train_loss=0.00122, valid_loss=0.00266]\n",
      "Epoch: 23/30: 100%|██████████| 83/83 [00:03<00:00, 25.19it/s, train_loss=0.00113, valid_loss=0.00278]\n",
      "Epoch: 24/30: 100%|██████████| 83/83 [00:03<00:00, 25.73it/s, train_loss=0.00116, valid_loss=0.0025]  \n",
      "Epoch: 25/30: 100%|██████████| 83/83 [00:03<00:00, 25.55it/s, train_loss=0.0014, valid_loss=0.00276]  \n",
      "Epoch: 26/30: 100%|██████████| 83/83 [00:03<00:00, 25.40it/s, train_loss=0.00116, valid_loss=0.00277]\n",
      "Epoch: 27/30: 100%|██████████| 83/83 [00:03<00:00, 25.72it/s, train_loss=0.00102, valid_loss=0.00281]\n",
      "Epoch: 28/30: 100%|██████████| 83/83 [00:03<00:00, 25.09it/s, train_loss=0.00112, valid_loss=0.0026]  \n",
      "Epoch: 29/30: 100%|██████████| 83/83 [00:03<00:00, 25.40it/s, train_loss=0.00108, valid_loss=0.0028]  \n",
      "Epoch: 30/30: 100%|██████████| 83/83 [00:03<00:00, 25.71it/s, train_loss=0.00113, valid_loss=0.0028]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: batch_size-256, lr-0.01, number_epoch-30, hidden_dim-256, n_layers-2,drop_prob-0,weight_decay-0\n",
      "cross-validation dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|██████████| 83/83 [00:02<00:00, 30.04it/s, train_loss=0.0341, valid_loss=0.0347]\n",
      "Epoch: 2/30: 100%|██████████| 83/83 [00:02<00:00, 31.30it/s, train_loss=0.0302, valid_loss=0.0355]\n",
      "Epoch: 3/30: 100%|██████████| 83/83 [00:02<00:00, 30.40it/s, train_loss=0.0304, valid_loss=0.0353]\n",
      "Epoch: 4/30: 100%|██████████| 83/83 [00:02<00:00, 32.12it/s, train_loss=0.0292, valid_loss=0.0364]\n",
      "Epoch: 5/30: 100%|██████████| 83/83 [00:02<00:00, 32.23it/s, train_loss=0.0313, valid_loss=0.0352]\n",
      "Epoch: 6/30: 100%|██████████| 83/83 [00:02<00:00, 30.45it/s, train_loss=0.0311, valid_loss=0.0363]\n",
      "Epoch: 7/30: 100%|██████████| 83/83 [00:02<00:00, 32.01it/s, train_loss=0.0305, valid_loss=0.0357]\n",
      "Epoch: 8/30: 100%|██████████| 83/83 [00:02<00:00, 27.68it/s, train_loss=0.0336, valid_loss=0.0396]\n",
      "Epoch: 9/30: 100%|██████████| 83/83 [00:02<00:00, 31.59it/s, train_loss=0.0291, valid_loss=0.0348]\n",
      "Epoch: 10/30: 100%|██████████| 83/83 [00:02<00:00, 29.25it/s, train_loss=0.0296, valid_loss=0.0355]\n",
      "Epoch: 11/30: 100%|██████████| 83/83 [00:02<00:00, 30.85it/s, train_loss=0.032, valid_loss=0.0357] \n",
      "Epoch: 12/30: 100%|██████████| 83/83 [00:02<00:00, 30.79it/s, train_loss=0.0315, valid_loss=0.0351]\n",
      "Epoch: 13/30: 100%|██████████| 83/83 [00:02<00:00, 32.49it/s, train_loss=0.0302, valid_loss=0.0353]\n",
      "Epoch: 14/30: 100%|██████████| 83/83 [00:02<00:00, 29.80it/s, train_loss=0.0303, valid_loss=0.0352]\n",
      "Epoch: 15/30: 100%|██████████| 83/83 [00:02<00:00, 30.74it/s, train_loss=0.0326, valid_loss=0.035] \n",
      "Epoch: 16/30: 100%|██████████| 83/83 [00:02<00:00, 31.38it/s, train_loss=0.0312, valid_loss=0.0356]\n",
      "Epoch: 17/30: 100%|██████████| 83/83 [00:02<00:00, 32.58it/s, train_loss=0.031, valid_loss=0.0333] \n",
      "Epoch: 18/30: 100%|██████████| 83/83 [00:02<00:00, 32.25it/s, train_loss=0.0313, valid_loss=0.0361]\n",
      "Epoch: 19/30: 100%|██████████| 83/83 [00:02<00:00, 31.44it/s, train_loss=0.0291, valid_loss=0.0347]\n",
      "Epoch: 20/30: 100%|██████████| 83/83 [00:02<00:00, 32.25it/s, train_loss=0.0294, valid_loss=0.035] \n",
      "Epoch: 21/30: 100%|██████████| 83/83 [00:02<00:00, 31.47it/s, train_loss=0.0314, valid_loss=0.0354]\n",
      "Epoch: 22/30: 100%|██████████| 83/83 [00:02<00:00, 32.06it/s, train_loss=0.0277, valid_loss=0.0354]\n",
      "Epoch: 23/30: 100%|██████████| 83/83 [00:02<00:00, 32.70it/s, train_loss=0.0306, valid_loss=0.0347]\n",
      "Epoch: 24/30: 100%|██████████| 83/83 [00:02<00:00, 31.46it/s, train_loss=0.031, valid_loss=0.0355] \n",
      "Epoch: 25/30: 100%|██████████| 83/83 [00:02<00:00, 32.51it/s, train_loss=0.0316, valid_loss=0.0351]\n",
      "Epoch: 26/30: 100%|██████████| 83/83 [00:02<00:00, 31.42it/s, train_loss=0.0304, valid_loss=0.0351]\n",
      "Epoch: 27/30: 100%|██████████| 83/83 [00:02<00:00, 32.37it/s, train_loss=0.0328, valid_loss=0.0352]\n",
      "Epoch: 28/30: 100%|██████████| 83/83 [00:02<00:00, 31.91it/s, train_loss=0.0344, valid_loss=0.0351]\n",
      "Epoch: 29/30: 100%|██████████| 83/83 [00:02<00:00, 31.45it/s, train_loss=0.0312, valid_loss=0.0358]\n",
      "Epoch: 30/30: 100%|██████████| 83/83 [00:02<00:00, 32.20it/s, train_loss=0.0296, valid_loss=0.0349]\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "torch.cuda.random.manual_seed(0)\n",
    "records = run_model_hpo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb16b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T08:52:06.844501Z",
     "start_time": "2021-12-22T08:52:06.830928Z"
    }
   },
   "source": [
    "## find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a205acd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T12:31:38.253463Z",
     "start_time": "2021-12-29T12:31:38.225481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>number_epoch</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>drop_prob</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size    lr  number_epoch  hidden_dim  n_layers  drop_prob  \\\n",
       "1         256  0.01            30         128         2          0   \n",
       "0         256  0.01            30          64         2          0   \n",
       "4         256  0.01            30          64         3          0   \n",
       "5         256  0.01            30         256         2          0   \n",
       "3         256  0.01            30         128         3          0   \n",
       "2         256  0.01            30         256         3          0   \n",
       "\n",
       "   weight_decay  valid_loss  \n",
       "1             0    0.002356  \n",
       "0             0    0.002439  \n",
       "4             0    0.002467  \n",
       "5             0    0.019121  \n",
       "3             0    0.034462  \n",
       "2             0    0.034503  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = pd.DataFrame(records).sort_values(by='valid_loss')\n",
    "records.to_csv('./records/LSTM_records.csv', mode='a', index=False, header=False)\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0f6a7",
   "metadata": {},
   "source": [
    "## retrain a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc70adbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:45.108882Z",
     "start_time": "2022-01-09T08:15:45.084860Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, valid_x, valid_y, input_size, output_size,\n",
    "                mse_thresh, batch_size, lr, number_epoch, hidden_dim, n_layers,\n",
    "                drop_prob, weight_decay):\n",
    "    while (1):\n",
    "        model = LSTM(input_size, output_size, hidden_dim, n_layers,\n",
    "                       drop_prob)\n",
    "        model.to(device=device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98)\n",
    "        valid_loss_min = np.Inf\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(train_x),\n",
    "                                      torch.FloatTensor(train_y))\n",
    "        valid_dataset = TensorDataset(torch.FloatTensor(valid_x),\n",
    "                                      torch.FloatTensor(valid_y))\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        num_without_imp = 0\n",
    "        train_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        # training process\n",
    "        for epoch in range(1, number_epoch + 1):\n",
    "            loop = tqdm(enumerate(train_loader),\n",
    "                        total=len(train_loader),\n",
    "                        leave=True, ncols=100)\n",
    "            for i, (inputs, labels) in loop:\n",
    "                inputs = inputs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if i % 5 == 0:\n",
    "                    num_without_imp = num_without_imp + 1\n",
    "                    valid_losses = list()\n",
    "                    model.eval()\n",
    "                    for inp, lab in valid_loader:\n",
    "                        inp = inp.to(device)\n",
    "                        lab = lab.to(device)\n",
    "                        out = model(inp)\n",
    "                        valid_loss = criterion(out, lab)\n",
    "                        valid_losses.append(valid_loss.item())\n",
    "\n",
    "                    model.train()\n",
    "                    loop.set_description(\"Epoch: {}/{}\".format(\n",
    "                        epoch, number_epoch))\n",
    "                    loop.set_postfix(train_loss=loss.item(),\n",
    "                                     valid_loss=np.mean(valid_losses))\n",
    "                    \n",
    "                    train_loss_list.append(loss.item())\n",
    "                    valid_loss_list.append(np.mean(valid_losses))\n",
    "                    if np.mean(valid_losses) < valid_loss_min:\n",
    "                        num_without_imp = 0\n",
    "                        torch.save(model.state_dict(),\n",
    "                                   './model/LSTM_state_dict.pt')\n",
    "                        valid_loss_min = np.mean(valid_losses)\n",
    "            scheduler.step()\n",
    "        if valid_loss_min < mse_thresh:\n",
    "            break\n",
    "    return model, train_loss_list, valid_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d169ee",
   "metadata": {},
   "source": [
    "## test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377b5622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:16.862729Z",
     "start_time": "2022-01-09T08:15:16.844829Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_x, test_y, scaler_y, batch_size):\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(test_x),\n",
    "                                 torch.FloatTensor(test_y))\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)\n",
    "    model.load_state_dict(torch.load('./model/LSTM_state_dict.pt'))\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, label in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            label = label.to(device)\n",
    "            outputs = model(inputs)\n",
    "            y_pred += outputs.cpu().numpy().flatten().tolist()\n",
    "            y_true += label.cpu().numpy().flatten().tolist()\n",
    "    y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "    y_true = np.array(y_true).reshape(-1, 1)\n",
    "#     pdb.set_trace()\n",
    "    load_pred = scaler_y.inverse_transform(y_pred)\n",
    "    load_true = scaler_y.inverse_transform(y_true)\n",
    "    \n",
    "    mean_pred = np.mean(load_pred)\n",
    "    mean_true = np.mean(load_true)\n",
    "    MAPE = np.mean(np.abs(load_true - load_pred) / load_true)\n",
    "    SMAPE = 2 * np.mean(\n",
    "        np.abs(load_true - load_pred) / (load_true + load_pred))\n",
    "    MAE = np.mean(np.abs(load_true - load_pred))\n",
    "    RMSE = np.sqrt(np.mean(np.square(load_true - load_pred)))\n",
    "    RRSE = np.sqrt(np.sum(np.square(load_true - load_pred))) / np.sqrt(\n",
    "        np.sum(np.square(load_true - mean_true)))\n",
    "    CORR = np.mean(\n",
    "        np.sum((load_true - mean_true) * (load_pred - mean_pred)) /\n",
    "        np.sqrt(np.sum(\n",
    "            (load_true - mean_true)**2 * (load_pred - mean_pred)**2)))\n",
    "    return MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba7a36",
   "metadata": {},
   "source": [
    "## run model retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03905cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:17.804677Z",
     "start_time": "2022-01-09T08:15:17.788633Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_retraining(seq_len=seq_len,\n",
    "                         target_len=target_len,\n",
    "                         mse_thresh=mse_thresh):\n",
    "    train_data = data[:int(0.8 * len(data))]\n",
    "    #     train_data = data[:-800]\n",
    "    train_data, scaler, scaler_y = normalization(train_data)\n",
    "    train_x, train_y = series_to_supervise(train_data, seq_len, target_len)\n",
    "\n",
    "    valid_x = train_x[int(0.8 * len(train_x)):]\n",
    "    valid_y = train_y[int(0.8 * len(train_y)):]\n",
    "    train_x = train_x[:int(0.8 * len(train_x))]\n",
    "    train_y = train_y[:int(0.8 * len(train_y))]\n",
    "    input_size = train_x.shape[2]\n",
    "    output_size = target_len\n",
    "\n",
    "    #     hyper-parameters define\n",
    "    batch_size = 512\n",
    "    lr = 0.01\n",
    "    number_epoch = 80\n",
    "    hidden_dim = 128\n",
    "    n_layers = 2\n",
    "    drop_prob = 0\n",
    "    weight_decay = 0\n",
    "    mse_thresh = 0.01\n",
    "\n",
    "    model, train_loss_list, valid_loss_list = train_model(\n",
    "        train_x, train_y, valid_x, valid_y, input_size, output_size,\n",
    "        mse_thresh, batch_size, lr, number_epoch, hidden_dim, n_layers,\n",
    "        drop_prob, weight_decay)\n",
    "    \n",
    "    # plot training process\n",
    "    plt.plot(train_loss_list[10:], 'm', label='train_loss')\n",
    "    plt.plot(valid_loss_list[10:], 'g', label='valid_loss')\n",
    "    plt.grid('both')\n",
    "    plt.legend()\n",
    "    \n",
    "    # test\n",
    "    test_data = data[int(0.8 * len(data)):]\n",
    "    #     test_data = data[-800:]\n",
    "    test_data = scaler.transform(test_data)\n",
    "    test_x, test_y = series_to_supervise(test_data, seq_len, target_len)\n",
    "    MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true = test_model(model, test_x, test_y,\n",
    "                                                       scaler_y, batch_size)\n",
    "    return MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ff99ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:17:29.330808Z",
     "start_time": "2022-01-09T08:15:49.325410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised data: shape of x: (25232, 72, 16), shape of y: (25232, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/80: 100%|████████████| 40/40 [00:01<00:00, 32.93it/s, train_loss=0.0299, valid_loss=0.0339]\n",
      "Epoch: 2/80: 100%|████████████| 40/40 [00:01<00:00, 30.24it/s, train_loss=0.0316, valid_loss=0.0339]\n",
      "Epoch: 3/80: 100%|████████████| 40/40 [00:01<00:00, 32.39it/s, train_loss=0.0316, valid_loss=0.0337]\n",
      "Epoch: 4/80: 100%|████████████| 40/40 [00:01<00:00, 30.58it/s, train_loss=0.0301, valid_loss=0.0331]\n",
      "Epoch: 5/80: 100%|████████████| 40/40 [00:01<00:00, 33.33it/s, train_loss=0.0316, valid_loss=0.0347]\n",
      "Epoch: 6/80: 100%|████████████| 40/40 [00:01<00:00, 30.62it/s, train_loss=0.0192, valid_loss=0.0122]\n",
      "Epoch: 7/80: 100%|██████████| 40/40 [00:01<00:00, 32.10it/s, train_loss=0.00249, valid_loss=0.00371]\n",
      "Epoch: 8/80: 100%|█████████████| 40/40 [00:01<00:00, 31.92it/s, train_loss=0.0021, valid_loss=0.003]\n",
      "Epoch: 9/80: 100%|██████████| 40/40 [00:01<00:00, 33.97it/s, train_loss=0.00206, valid_loss=0.00364]\n",
      "Epoch: 10/80: 100%|█████████| 40/40 [00:01<00:00, 34.34it/s, train_loss=0.00179, valid_loss=0.00287]\n",
      "Epoch: 11/80: 100%|█████████| 40/40 [00:01<00:00, 32.49it/s, train_loss=0.00196, valid_loss=0.00285]\n",
      "Epoch: 12/80: 100%|█████████| 40/40 [00:01<00:00, 34.16it/s, train_loss=0.00172, valid_loss=0.00279]\n",
      "Epoch: 13/80: 100%|█████████| 40/40 [00:01<00:00, 33.00it/s, train_loss=0.00187, valid_loss=0.00249]\n",
      "Epoch: 14/80: 100%|█████████| 40/40 [00:01<00:00, 32.44it/s, train_loss=0.00152, valid_loss=0.00255]\n",
      "Epoch: 15/80: 100%|█████████| 40/40 [00:01<00:00, 30.45it/s, train_loss=0.00179, valid_loss=0.00247]\n",
      "Epoch: 16/80: 100%|█████████| 40/40 [00:01<00:00, 34.34it/s, train_loss=0.00152, valid_loss=0.00237]\n",
      "Epoch: 17/80: 100%|█████████| 40/40 [00:01<00:00, 32.60it/s, train_loss=0.00138, valid_loss=0.00234]\n",
      "Epoch: 18/80: 100%|█████████| 40/40 [00:01<00:00, 30.37it/s, train_loss=0.00151, valid_loss=0.00264]\n",
      "Epoch: 19/80: 100%|█████████| 40/40 [00:01<00:00, 32.33it/s, train_loss=0.00141, valid_loss=0.00241]\n",
      "Epoch: 20/80: 100%|██████████| 40/40 [00:01<00:00, 32.28it/s, train_loss=0.0015, valid_loss=0.00234]\n",
      "Epoch: 21/80: 100%|█████████| 40/40 [00:01<00:00, 33.84it/s, train_loss=0.00137, valid_loss=0.00239]\n",
      "Epoch: 22/80: 100%|█████████| 40/40 [00:01<00:00, 31.01it/s, train_loss=0.00146, valid_loss=0.00239]\n",
      "Epoch: 23/80: 100%|█████████| 40/40 [00:01<00:00, 32.03it/s, train_loss=0.00127, valid_loss=0.00237]\n",
      "Epoch: 24/80: 100%|███████████| 40/40 [00:01<00:00, 34.36it/s, train_loss=0.0013, valid_loss=0.0023]\n",
      "Epoch: 25/80: 100%|██████████| 40/40 [00:01<00:00, 31.89it/s, train_loss=0.00115, valid_loss=0.0023]\n",
      "Epoch: 26/80: 100%|█████████| 40/40 [00:01<00:00, 33.03it/s, train_loss=0.00127, valid_loss=0.00224]\n",
      "Epoch: 27/80: 100%|█████████| 40/40 [00:01<00:00, 31.93it/s, train_loss=0.00116, valid_loss=0.00236]\n",
      "Epoch: 28/80: 100%|██████████| 40/40 [00:01<00:00, 31.59it/s, train_loss=0.00139, valid_loss=0.0025]\n",
      "Epoch: 29/80: 100%|████████| 40/40 [00:01<00:00, 30.99it/s, train_loss=0.000996, valid_loss=0.00262]\n",
      "Epoch: 30/80: 100%|██████████| 40/40 [00:01<00:00, 34.49it/s, train_loss=0.0012, valid_loss=0.00251]\n",
      "Epoch: 31/80: 100%|█████████| 40/40 [00:01<00:00, 32.45it/s, train_loss=0.00114, valid_loss=0.00259]\n",
      "Epoch: 32/80: 100%|█████████| 40/40 [00:01<00:00, 33.57it/s, train_loss=0.00111, valid_loss=0.00278]\n",
      "Epoch: 33/80: 100%|█████████| 40/40 [00:01<00:00, 33.62it/s, train_loss=0.00103, valid_loss=0.00267]\n",
      "Epoch: 34/80: 100%|█████████| 40/40 [00:01<00:00, 33.01it/s, train_loss=0.00104, valid_loss=0.00266]\n",
      "Epoch: 35/80: 100%|████████| 40/40 [00:01<00:00, 33.77it/s, train_loss=0.000924, valid_loss=0.00282]\n",
      "Epoch: 36/80: 100%|████████| 40/40 [00:01<00:00, 32.58it/s, train_loss=0.000936, valid_loss=0.00249]\n",
      "Epoch: 37/80: 100%|████████| 40/40 [00:01<00:00, 32.85it/s, train_loss=0.000921, valid_loss=0.00279]\n",
      "Epoch: 38/80: 100%|██████████| 40/40 [00:01<00:00, 32.39it/s, train_loss=0.00086, valid_loss=0.0028]\n",
      "Epoch: 39/80: 100%|█████████| 40/40 [00:01<00:00, 34.16it/s, train_loss=0.00118, valid_loss=0.00251]\n",
      "Epoch: 40/80: 100%|████████| 40/40 [00:01<00:00, 31.95it/s, train_loss=0.000938, valid_loss=0.00265]\n",
      "Epoch: 41/80: 100%|████████| 40/40 [00:01<00:00, 31.53it/s, train_loss=0.000896, valid_loss=0.00258]\n",
      "Epoch: 42/80: 100%|████████| 40/40 [00:01<00:00, 33.32it/s, train_loss=0.000771, valid_loss=0.00257]\n",
      "Epoch: 43/80: 100%|████████| 40/40 [00:01<00:00, 32.29it/s, train_loss=0.000719, valid_loss=0.00253]\n",
      "Epoch: 44/80: 100%|█████████| 40/40 [00:01<00:00, 33.12it/s, train_loss=0.00068, valid_loss=0.00263]\n",
      "Epoch: 45/80: 100%|████████| 40/40 [00:01<00:00, 31.19it/s, train_loss=0.000651, valid_loss=0.00258]\n",
      "Epoch: 46/80: 100%|████████| 40/40 [00:01<00:00, 33.69it/s, train_loss=0.000682, valid_loss=0.00278]\n",
      "Epoch: 47/80: 100%|████████| 40/40 [00:01<00:00, 32.15it/s, train_loss=0.000628, valid_loss=0.00265]\n",
      "Epoch: 48/80: 100%|████████| 40/40 [00:01<00:00, 33.48it/s, train_loss=0.000617, valid_loss=0.00277]\n",
      "Epoch: 49/80: 100%|█████████| 40/40 [00:01<00:00, 29.92it/s, train_loss=0.000503, valid_loss=0.0027]\n",
      "Epoch: 50/80: 100%|████████| 40/40 [00:01<00:00, 33.50it/s, train_loss=0.000524, valid_loss=0.00265]\n",
      "Epoch: 51/80: 100%|████████| 40/40 [00:01<00:00, 33.94it/s, train_loss=0.000521, valid_loss=0.00273]\n",
      "Epoch: 52/80: 100%|████████| 40/40 [00:01<00:00, 28.91it/s, train_loss=0.000532, valid_loss=0.00268]\n",
      "Epoch: 53/80: 100%|████████| 40/40 [00:01<00:00, 31.72it/s, train_loss=0.000582, valid_loss=0.00266]\n",
      "Epoch: 54/80: 100%|████████| 40/40 [00:01<00:00, 29.46it/s, train_loss=0.000457, valid_loss=0.00277]\n",
      "Epoch: 55/80: 100%|████████| 40/40 [00:01<00:00, 31.83it/s, train_loss=0.000448, valid_loss=0.00267]\n",
      "Epoch: 56/80: 100%|████████| 40/40 [00:01<00:00, 29.81it/s, train_loss=0.000459, valid_loss=0.00269]\n",
      "Epoch: 57/80: 100%|████████| 40/40 [00:01<00:00, 31.93it/s, train_loss=0.000446, valid_loss=0.00269]\n",
      "Epoch: 58/80: 100%|████████| 40/40 [00:01<00:00, 30.63it/s, train_loss=0.000392, valid_loss=0.00272]\n",
      "Epoch: 59/80: 100%|████████| 40/40 [00:01<00:00, 30.96it/s, train_loss=0.000348, valid_loss=0.00283]\n",
      "Epoch: 60/80: 100%|████████| 40/40 [00:01<00:00, 30.73it/s, train_loss=0.000342, valid_loss=0.00287]\n",
      "Epoch: 61/80: 100%|████████| 40/40 [00:01<00:00, 31.21it/s, train_loss=0.000367, valid_loss=0.00288]\n",
      "Epoch: 62/80: 100%|████████| 40/40 [00:01<00:00, 30.87it/s, train_loss=0.000371, valid_loss=0.00283]\n",
      "Epoch: 63/80: 100%|████████| 40/40 [00:01<00:00, 30.11it/s, train_loss=0.000313, valid_loss=0.00277]\n",
      "Epoch: 64/80: 100%|████████| 40/40 [00:01<00:00, 31.54it/s, train_loss=0.000304, valid_loss=0.00283]\n",
      "Epoch: 65/80: 100%|████████| 40/40 [00:01<00:00, 30.28it/s, train_loss=0.000287, valid_loss=0.00286]\n",
      "Epoch: 66/80: 100%|████████| 40/40 [00:01<00:00, 31.59it/s, train_loss=0.000266, valid_loss=0.00289]\n",
      "Epoch: 67/80: 100%|████████| 40/40 [00:01<00:00, 30.31it/s, train_loss=0.000245, valid_loss=0.00299]\n",
      "Epoch: 68/80: 100%|████████| 40/40 [00:01<00:00, 31.65it/s, train_loss=0.000263, valid_loss=0.00291]\n",
      "Epoch: 69/80: 100%|████████| 40/40 [00:01<00:00, 33.20it/s, train_loss=0.000236, valid_loss=0.00291]\n",
      "Epoch: 70/80: 100%|████████| 40/40 [00:01<00:00, 33.47it/s, train_loss=0.000274, valid_loss=0.00287]\n",
      "Epoch: 71/80: 100%|█████████| 40/40 [00:01<00:00, 33.97it/s, train_loss=0.00023, valid_loss=0.00288]\n",
      "Epoch: 72/80: 100%|████████| 40/40 [00:01<00:00, 32.96it/s, train_loss=0.000218, valid_loss=0.00294]\n",
      "Epoch: 73/80: 100%|████████| 40/40 [00:01<00:00, 33.69it/s, train_loss=0.000207, valid_loss=0.00302]\n",
      "Epoch: 74/80: 100%|████████| 40/40 [00:01<00:00, 32.77it/s, train_loss=0.000192, valid_loss=0.00304]\n",
      "Epoch: 75/80: 100%|████████| 40/40 [00:01<00:00, 34.30it/s, train_loss=0.000209, valid_loss=0.00299]\n",
      "Epoch: 76/80: 100%|████████| 40/40 [00:01<00:00, 32.44it/s, train_loss=0.000178, valid_loss=0.00299]\n",
      "Epoch: 77/80: 100%|████████| 40/40 [00:01<00:00, 34.31it/s, train_loss=0.000167, valid_loss=0.00297]\n",
      "Epoch: 78/80: 100%|████████| 40/40 [00:01<00:00, 34.18it/s, train_loss=0.000168, valid_loss=0.00304]\n",
      "Epoch: 79/80: 100%|████████| 40/40 [00:01<00:00, 32.64it/s, train_loss=0.000174, valid_loss=0.00302]\n",
      "Epoch: 80/80: 100%|████████| 40/40 [00:01<00:00, 33.00it/s, train_loss=0.000153, valid_loss=0.00306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised data: shape of x: (6237, 72, 16), shape of y: (6237, 24)\n",
      "MAPE:0.036662,SMAPE:0.036339,MAE:478.826866,RMSE:708.689894,RRSE:0.219272,CORR:258.646905\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "torch.cuda.random.manual_seed(0)\n",
    "MAPE, SMAPE, MAE, RMSE, RRSE, CORR, load_pred, load_true = run_model_retraining()\n",
    "print('MAPE:{:.6f},SMAPE:{:.6f},MAE:{:.6f},RMSE:{:.6f},RRSE:{:.6f},CORR:{:.6f}'.format(MAPE, SMAPE, MAE, RMSE, RRSE, CORR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e244f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:29.561194Z",
     "start_time": "2022-01-09T08:15:29.561185Z"
    }
   },
   "outputs": [],
   "source": [
    "print('MAPE:{:.6f},SMAPE:{:.6f},MAE:{:.6f},RMSE:{:.6f},RRSE:{:.6f},CORR:{:.6f}'.format(MAPE, SMAPE, MAE, RMSE, RRSE, CORR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8e819",
   "metadata": {},
   "source": [
    "## figure plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c3efa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T08:15:29.562244Z",
     "start_time": "2022-01-09T08:15:29.562235Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "load_pred = load_pred.reshape(-1, target_len)\n",
    "load_true = load_true.reshape(-1, target_len)\n",
    "plt.plot(load_pred[:240,0], 'r')\n",
    "plt.plot(load_true[:240,0], 'g')\n",
    "plt.ylim(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 320.844,
   "position": {
    "height": "342.844px",
    "left": "1482px",
    "right": "20px",
    "top": "127px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
